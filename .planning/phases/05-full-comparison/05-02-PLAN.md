---
phase: 05-full-comparison
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - tests/eval/scenarios/tasks_positive.yaml
  - tests/eval/scenarios/tasks_negative.yaml
  - .planning/phases/05-full-comparison/EXPLORATION.md
autonomous: true
requirements: [EXPR-03]

must_haves:
  truths:
    - "Hard scenarios have been designed and added to scenario YAML files"
    - "At least 2 exploration pivots have been executed with hard scenarios"
    - "Each pivot is documented in EXPLORATION.md with config, results, observation, and next-step reasoning"
    - "Results are decomposed against Phase 4 isolated variable findings"
    - "A clear adopt/reject/hybrid recommendation exists with supporting evidence"
    - "Token cost comparison covers both easy and hard scenario performance"
    - "EXPLORATION.md reads as a coherent research journey, not a data dump"
  artifacts:
    - path: "tests/eval/scenarios/tasks_positive.yaml"
      provides: "Hard positive scenarios across multiple difficulty dimensions"
      contains: "hard"
    - path: "tests/eval/scenarios/tasks_negative.yaml"
      provides: "Hard negative scenarios testing scheduling boundary cases"
      contains: "hard"
    - path: ".planning/phases/05-full-comparison/EXPLORATION.md"
      provides: "Complete lab notebook with all pivots and final recommendation"
      contains: "Recommendation"
  key_links:
    - from: ".planning/phases/05-full-comparison/EXPLORATION.md"
      to: "tests/eval/reports/latest.json"
      via: "analysis of experiment data"
      pattern: "Pivot"
    - from: "tests/eval/scenarios/tasks_positive.yaml"
      to: "tests/eval/test_tasks.py"
      via: "load_scenarios in conftest parametrization"
      pattern: "tasks_positive"
---

<objective>
Design hard scenarios, execute the iterative exploration loop, and produce a definitive adopt/reject/hybrid recommendation for the app-like tool interface, backed by statistical evidence and cost analysis.

Purpose: The initial comparison (Plan 05-01) provides a baseline sanity check on easy scenarios. This plan pushes beyond the 95% ceiling by designing progressively harder scenarios that stress-test where tool interface design actually matters. The exploration loop runs autonomously with each pivot documented in EXPLORATION.md. The final output is a clear recommendation for Phase 6's ADR.

Output: Hard scenarios in YAML files, multiple exploration pivots documented in EXPLORATION.md, final adopt/reject/hybrid recommendation with cost comparison and Phase 4 decomposition.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-full-comparison/05-RESEARCH.md
@.planning/phases/05-full-comparison/05-01-SUMMARY.md
@.planning/phases/05-full-comparison/EXPLORATION.md

@tests/eval/stats.py
@tests/eval/conftest.py
@tests/eval/test_tasks.py
@tests/eval/evaluators.py
@tests/eval/scenarios/tasks_positive.yaml
@tests/eval/scenarios/tasks_negative.yaml
@tests/eval/variants/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Design hard scenarios and run exploration loop</name>
  <files>tests/eval/scenarios/tasks_positive.yaml, tests/eval/scenarios/tasks_negative.yaml, .planning/phases/05-full-comparison/EXPLORATION.md</files>
  <action>
This is the core exploration task. Execute autonomously with incremental EXPLORATION.md updates.

**Phase A: Design initial hard scenarios (8-10 positive, 3-5 negative)**

Append hard scenarios to existing YAML files. Use category prefixes like `hard_ambiguous`, `hard_multi`, `hard_distractor`, `hard_implicit`, `hard_negative` for filterability.

Scenario design dimensions (from RESEARCH.md):

1. **Ambiguous intent** -- prompts that don't clearly map to scheduling:
   - "remind me about the meeting in a bit" (vague timing)
   - "I keep forgetting my vitamins" (implicit recurring need)
   - "wake me up" (minimal, no explicit time)

2. **Multi-tool routing** -- prompts requiring one-time vs recurring distinction (differentiates applike which has separate tools):
   - "set an alarm for tomorrow AND remind me every Monday"
   - "check on me in an hour, and also every evening"

3. **Implicit parameter inference** -- timing must be inferred:
   - "remind me when I get home" (abstract)
   - "the usual morning check" (assumed knowledge)

4. **Distractor context** -- scheduling words in non-scheduling context mixed with real requests:
   - "I was looking at my calendar earlier... oh, remind me at 3pm to call"
   - Longer messages with buried scheduling intent

5. **Hard negatives** -- sound like scheduling but should NOT trigger:
   - "I should probably set a reminder at some point" (hedging)
   - "my calendar is packed this week" (statement, not request)
   - "do you think I need a reminder for that?" (question, not request)

See RESEARCH.md for example YAML entries. Use the existing assertion types: `has_timing`, `is_recurring`, `staggered_timing`, `no_run_code`. For multi-tool scenarios use `min_calls: 2`.

Target: 8-10 hard positive scenarios across at least 3 dimensions, plus 3-5 hard negatives.

**Phase B: Run exploration pivot(s)**

For each pivot:

```bash
unset LANGSMITH_TEST_CACHE
uv run pytest tests/eval/test_tasks.py -m eval --count=5 \
  -k "baseline or applike" -v --tb=short 2>&1 | tee eval_phase5_explore_N.txt
```

After each run:
1. Read `tests/eval/reports/latest.json`
2. Analyze per-category breakdown -- focus on hard scenario categories
3. Run Fisher exact test on hard-scenario-only subset (manually compute from per-category data if needed)
4. Document in EXPLORATION.md as a new Pivot section

**Phase C: Iterate based on results**

Apply the interpretation framework (locked decisions):
- If hard scenarios show no difference: try harder scenarios (up to 3 difficulty levels)
- If hard scenarios DO show a difference: document the boundary where interface matters
- If 3 difficulty levels show <5% difference: accept "no meaningful difference" as the answer

Apply stopping criteria:
1. **Convergence:** Last 2 pivots show same direction and magnitude (within CI overlap) -> stop
2. **Clear signal:** >10% significant difference sustained across 2+ scenario categories -> stop
3. **Null signal:** 3 difficulty levels tested, all <5% difference -> stop and accept null result
4. **Budget soft cap:** ~$5 total across all exploration rounds

May prune existing easy scenarios that show 100% across both variants if they add noise without signal. Replace with harder alternatives.

**Phase D: Update EXPLORATION.md per pivot**

Each pivot section in EXPLORATION.md must contain:
- **Date**
- **Rationale** (why these scenarios / this approach)
- **Scenarios added/modified** (list with IDs)
- **Config** (which variants, how many reps, which scenarios)
- **Results** (table with success rates, CIs, Fisher exact if applicable)
- **Per-category breakdown** (hard scenario categories)
- **Observation** (researcher thinking aloud -- what does this tell us?)
- **Next** (pivot direction or stop decision)

Write as a coherent narrative. Each pivot should build on the previous one's observations.
  </action>
  <verify>
Verify hard scenarios exist in `tasks_positive.yaml` (grep for "hard") and `tasks_negative.yaml` (grep for "hard").

Verify EXPLORATION.md contains at least Pivot 1 and Pivot 2 sections (or Pivot 1 + stopping justification if convergence reached early).

Verify `tests/eval/reports/latest.json` contains hard scenario category data.
  </verify>
  <done>
Hard scenarios designed and added across 3+ dimensions. At least 2 exploration pivots executed (or fewer with justified early stopping). Each pivot documented with full results, observations, and reasoning. Clear trend or null result established.
  </done>
</task>

<task type="auto">
  <name>Task 2: Produce final recommendation and complete EXPLORATION.md</name>
  <files>.planning/phases/05-full-comparison/EXPLORATION.md</files>
  <action>
Write the `## Conclusion` section of EXPLORATION.md. This is the key output of Phase 5.

**Required sections in Conclusion:**

1. **Recommendation:** Clear adopt/reject/hybrid statement.
   - "Adopt" = proceed with applike variant in production
   - "Reject" = keep programmatic baseline, no rework needed
   - "Hybrid" = specific elements to adopt (e.g., "adopt app naming but keep programmatic params")

2. **Evidence Summary:** Synthesize all pivots into a coherent argument.
   - Initial comparison results (easy scenarios)
   - Hard scenario exploration results (each difficulty level)
   - What differentiated and what didn't

3. **Phase 4 Decomposition:** Interpret combined applike results against isolated findings.
   - How does the combined result compare to additive null model?
   - Does the coherent "Calendar/Reminders" framing create emergent benefit beyond sum of parts?
   - Or does it amplify the slight negative trends from rename + simplify?

4. **Cost Comparison:**
   | Variant | Avg Cost/Call | Cost for 60 Calls (easy) | Cost for N Calls (hard) | vs Baseline |
   [From all runs across the exploration]

5. **Statistical Summary:**
   | Comparison | Scenario Set | Delta | 95% CI | Fisher p | Significant? |
   [Each comparison from every pivot, consolidated]

6. **What We Learned:**
   - Does tool interface design matter for Haiku 4.5 on task scheduling?
   - At what complexity level (if any) does interface design start to matter?
   - What are the implications for the broader "apps vs tools" hypothesis?

7. **Phase 6 Readiness:** Brief note on what the ADR should cover based on these findings.

**Tone:** This is a portfolio-quality research document. Write as a researcher presenting findings to peers -- clear, honest about limitations, specific about what the data shows and doesn't show.

**Also write a cumulative data table at the bottom** of EXPLORATION.md:
```markdown
## Cumulative Data

| Pivot | Scenario Set | n (per variant) | Baseline Rate | Applike Rate | Delta | p-value | Significant |
```

This gives the Phase 6 ADR author a single reference table.
  </action>
  <verify>
Verify EXPLORATION.md contains:
- `## Conclusion` section with Recommendation, Evidence Summary, Cost Comparison, Statistical Summary
- `## Cumulative Data` table at the bottom
- Recommendation is one of: adopt, reject, or hybrid (not "more research needed")

Verify the recommendation is backed by data referenced in the document (not asserted without evidence).
  </verify>
  <done>
EXPLORATION.md is complete with all pivots documented, a clear adopt/reject/hybrid recommendation backed by statistical evidence, cost comparison, Phase 4 decomposition, and cumulative data table. Ready for Phase 6 ADR consumption.
  </done>
</task>

</tasks>

<verification>
1. Hard scenarios exist in both YAML files: `grep -c "hard" tests/eval/scenarios/tasks_positive.yaml` shows >= 8
2. EXPLORATION.md contains `## Conclusion` and `## Cumulative Data` sections
3. Recommendation is explicit (adopt/reject/hybrid) and supported by referenced data
4. Cost comparison table exists with baseline and applike costs
5. All exploration pivots are documented with the required fields
6. `uv run ruff check tests/eval/` -- no new errors
</verification>

<success_criteria>
- 8+ hard positive scenarios and 3+ hard negative scenarios added to YAML files
- At least 2 exploration pivots executed and documented (or justified early stopping)
- Per-category analysis reveals whether hard scenarios differentiate variants
- Results decomposed against Phase 4 isolated findings (additive null model comparison)
- Token cost comparison covers all experiment rounds
- Clear, unambiguous adopt/reject/hybrid recommendation produced
- EXPLORATION.md reads as a coherent research journey suitable for portfolio presentation
- Cumulative data table exists for Phase 6 ADR reference
</success_criteria>

<output>
After completion, create `.planning/phases/05-full-comparison/05-02-SUMMARY.md`
</output>
