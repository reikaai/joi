---
phase: 05-full-comparison
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/eval/stats.py
  - tests/eval/conftest.py
  - .planning/phases/05-full-comparison/EXPLORATION.md
autonomous: true
requirements: [EXPR-03]

must_haves:
  truths:
    - "Fisher exact test is available and callable from stats module"
    - "Report generation produces per-category breakdown alongside aggregate stats"
    - "Initial applike-vs-baseline comparison has been run with 5 reps on existing 12 scenarios"
    - "EXPLORATION.md Pivot 0 documents the initial comparison results and observations"
    - "Token cost comparison exists for baseline vs applike"
  artifacts:
    - path: "tests/eval/stats.py"
      provides: "Fisher exact test function"
      contains: "fisher_exact"
    - path: "tests/eval/conftest.py"
      provides: "Per-category breakdown in report generation"
      contains: "category"
    - path: ".planning/phases/05-full-comparison/EXPLORATION.md"
      provides: "Living lab notebook with Pivot 0 initial comparison"
      contains: "Pivot 0"
    - path: "tests/eval/reports/latest.json"
      provides: "Applike-vs-baseline comparison data"
  key_links:
    - from: "tests/eval/stats.py"
      to: "scipy.stats.fisher_exact"
      via: "import and wrapper function"
      pattern: "fisher_exact"
    - from: "tests/eval/conftest.py"
      to: "tests/eval/stats.py"
      via: "generate_report call"
      pattern: "generate_report"
---

<objective>
Add Fisher exact test to the statistical toolkit, enhance report generation with per-category breakdowns, run the initial applike-vs-baseline comparison on existing scenarios, and initialize EXPLORATION.md with Pivot 0 results.

Purpose: Establish the statistical baseline and sanity-check the applike variant against existing easy scenarios before diving into the hard-scenario exploration loop. Per power analysis, no significant difference is expected at the 95% baseline ceiling -- this is a calibration step.

Output: Enhanced stats.py with Fisher exact, enhanced report with category breakdown, initial comparison data in latest.json, EXPLORATION.md Pivot 0 documenting observations and next steps.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-full-comparison/05-RESEARCH.md
@.planning/phases/04-isolated-variable-experiments/04-02-SUMMARY.md

@tests/eval/stats.py
@tests/eval/conftest.py
@tests/eval/test_tasks.py
@tests/eval/evaluators.py
@tests/eval/scenarios/tasks_positive.yaml
@tests/eval/scenarios/tasks_negative.yaml
@tests/eval/variants/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Fisher exact test and per-category report breakdown</name>
  <files>tests/eval/stats.py, tests/eval/conftest.py</files>
  <action>
**stats.py additions:**

1. Add `fisher_exact_comparison()` function. Import `fisher_exact` from `scipy.stats`. Takes two `list[float]` score arrays, constructs a 2x2 contingency table (pass/fail x variant_a/variant_b), returns dict with `table`, `odds_ratio`, `p_value`, `significant` (p < 0.05). Use `alternative="two-sided"`. See RESEARCH.md code example.

2. Modify `generate_report()` to:
   a. Accept the Fisher exact results in the comparison loop: after computing `compare_variants()` for each pair, also call `fisher_exact_comparison(a_scores, b_scores)` and include the result under a `"fisher_exact"` key in each comparison dict.
   b. Add a `"by_category"` section to each variant's report entry. Group results by the `"category"` field already present in each result dict. For each category, compute `bootstrap_ci()` on that category's scores. Structure: `report["variants"][name]["by_category"] = { category: bootstrap_ci(scores) }`.

**conftest.py:** No changes needed -- `record_eval_result` already stores `category` in the result dict, and `generate_report` receives the full results list.

Do NOT modify any other files. Do NOT add new dependencies (scipy already installed, fisher_exact already available).
  </action>
  <verify>
Run `uv run python -c "from tests.eval.stats import fisher_exact_comparison; print(fisher_exact_comparison([1,1,1,0], [1,0,0,0]))"` -- should return a dict with p_value, odds_ratio, significant fields.

Run `uv run ruff check tests/eval/stats.py tests/eval/conftest.py` -- no errors.
  </verify>
  <done>
fisher_exact_comparison() exists and returns correct results. generate_report() produces per-category breakdowns and Fisher exact results in comparisons. Linting passes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run initial applike-vs-baseline comparison and write Pivot 0</name>
  <files>.planning/phases/05-full-comparison/EXPLORATION.md</files>
  <action>
**Run the experiment:**

```bash
# Fresh LLM calls -- no cache
unset LANGSMITH_TEST_CACHE
uv run pytest tests/eval/test_tasks.py -m eval --count=5 \
  -k "baseline or applike" -v --tb=short 2>&1 | tee eval_phase5_initial.txt
```

This runs: 2 variants (baseline, applike) x 12 scenarios (7 positive + 5 negative) x 5 reps = 120 LLM calls.

**Analyze results:**

1. Read `tests/eval/reports/latest.json` for the auto-generated report.
2. Extract: success rates with CIs for both variants, Fisher exact p-value, per-category breakdowns, token cost comparison.
3. Compute the additive null model from Phase 4 data:
   - Phase 4 isolated deltas: rename -3.3%, simplify -3.3%, description 0%
   - Additive expected: 95% - 6.6% = 88.4%
   - Compare actual applike result against this prediction.

**Write EXPLORATION.md:**

Create `.planning/phases/05-full-comparison/EXPLORATION.md` with this structure:

```markdown
# Phase 5: Full Comparison Exploration

## Overview
[Brief: what this document tracks, the two-phase approach]

## Phase 4 Reference Data
| Variant | Success Rate | 95% CI | Delta vs Baseline |
[Include baseline, rename, simplify, desc_a, desc_b from Phase 4]

## Pivot 0: Initial Comparison (Existing Scenarios)
**Date:** [today]
**Config:** applike vs baseline, 12 scenarios (7 positive + 5 negative), 5 reps each
**Results:**
| Variant | Success Rate | 95% CI | n |
[From latest.json]

**Fisher Exact Test:** p=[value], odds_ratio=[value], significant=[yes/no]

**Per-Category Breakdown:**
| Category | Baseline | Applike | Delta |
[From by_category in latest.json]

**Cost Comparison:**
| Variant | Avg Cost/Call | Cost for 60 Calls | vs Baseline |
[From latest.json cost_usd]

**Additive Null Model:**
- Expected applike under additive model: [value]
- Actual applike: [value]
- Interpretation: [matches additive / positive synergy / negative interaction]

**Observation:** [What we learned -- expected: no significant difference at this ceiling]
**Next:** [Proceed to hard scenario design -- which dimensions to explore first]
```

The EXPLORATION.md should read as a coherent research narrative, not a data dump. Write observations as a researcher thinking aloud about what the results mean and what to try next.
  </action>
  <verify>
Verify `tests/eval/reports/latest.json` exists and contains both `baseline` and `applike` variant data with `by_category` sections and Fisher exact results in comparisons.

Verify `.planning/phases/05-full-comparison/EXPLORATION.md` exists and contains the Pivot 0 section with results table, Fisher test, per-category breakdown, cost comparison, and next-steps reasoning.
  </verify>
  <done>
120 LLM calls completed (2 variants x 12 scenarios x 5 reps). EXPLORATION.md Pivot 0 documents the initial comparison including success rates, CIs, Fisher exact test, per-category breakdown, cost comparison, and additive null model interpretation. Clear next-step reasoning points toward hard scenario design.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from tests.eval.stats import fisher_exact_comparison; print('OK')"` -- imports without error
2. `tests/eval/reports/latest.json` contains `by_category` and `fisher_exact` fields
3. `.planning/phases/05-full-comparison/EXPLORATION.md` contains `## Pivot 0` section
4. `uv run ruff check tests/eval/stats.py` -- clean
</verification>

<success_criteria>
- Fisher exact test added to stats module and producing correct p-values
- Report generation includes per-category breakdowns for granular analysis
- Initial comparison (120 LLM calls) completed and results captured
- EXPLORATION.md Pivot 0 contains: success rates + CIs, Fisher exact, per-category breakdown, cost comparison, additive null model interpretation, and reasoned next steps
- All results interpretable against Phase 4 isolated findings
</success_criteria>

<output>
After completion, create `.planning/phases/05-full-comparison/05-01-SUMMARY.md`
</output>
