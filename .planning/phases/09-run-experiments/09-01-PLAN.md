---
phase: 09-run-experiments
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/experiment/capture.py
  - tests/experiment/conftest.py
  - tests/experiment/test_experiment.py
  - pyproject.toml
autonomous: true
requirements:
  - ANLS-01

must_haves:
  truths:
    - "Both tool variants (baseline, applike) produce per-run JSONL files with full response text, tool calls, and metadata"
    - "3 runs per variant exist (6 total JSONL files) with all 20 scenarios in each"
    - "LangSmith traces for every scenario execution include variant, run_id, and rep number"
    - "Prior run files are never overwritten — timestamped filenames preserve history"
    - "Temperature is set to 0.2 for consistent-but-slightly-varied behavior"
  artifacts:
    - path: "results/*.jsonl"
      provides: "6 JSONL files (2 variants x 3 runs) with 20 scenario results each"
    - path: "tests/experiment/capture.py"
      provides: "JSONLWriter with optional filename parameter for per-variant per-run output"
    - path: "tests/experiment/conftest.py"
      provides: "WriterPool, rep_number fixture, EVAL_TEMPERATURE constant, summary hook"
    - path: "tests/experiment/test_experiment.py"
      provides: "Test using writer_pool and rep_number, temperature on ChatAnthropic"
  key_links:
    - from: "tests/experiment/test_experiment.py"
      to: "tests/experiment/conftest.py"
      via: "writer_pool fixture and rep_number fixture"
      pattern: "writer_pool\\.get\\(variant_name.*rep_number\\)"
    - from: "tests/experiment/conftest.py"
      to: "tests/experiment/capture.py"
      via: "WriterPool creates JSONLWriter instances with per-run filenames"
      pattern: "JSONLWriter.*filename="
---

<objective>
Adapt the Phase 8 experiment harness for multi-run execution (3 runs x 2 variants = 120 LLM calls), run the full experiment, and validate completeness.

Purpose: Produce clean experiment data (ANLS-01) — 6 JSONL files + LangSmith traces covering all scenarios across 3 runs for both tool variants.
Output: Per-variant per-run JSONL files in results/ directory, LangSmith traces annotated with rep numbers.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-run-experiments/09-RESEARCH.md

@tests/experiment/capture.py
@tests/experiment/conftest.py
@tests/experiment/test_experiment.py
@tests/experiment/scenarios.py
@tests/experiment/variants/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Adapt harness for multi-run execution with per-variant per-run JSONL output</name>
  <files>
    tests/experiment/capture.py
    tests/experiment/conftest.py
    tests/experiment/test_experiment.py
    pyproject.toml
  </files>
  <action>
Install pytest-rerunfailures: `uv add --dev pytest-rerunfailures`

Modify `tests/experiment/capture.py`:
- Add optional `filename` parameter to `JSONLWriter.__init__()`. When provided, use it directly instead of generating one from run_id + timestamp. When not provided, keep current behavior (backward compat). Use append mode ("a") instead of write mode ("w") for crash safety.

Modify `tests/experiment/conftest.py`:
- Add `EVAL_TEMPERATURE = 0.2` constant alongside existing EVAL_MODEL.
- Create a `WriterPool` class that lazily creates per-variant per-run `JSONLWriter` instances keyed by `(variant_name, rep_number)`. Each writer gets filename like `{variant}_run{rep}_{timestamp}.jsonl`. The pool's `__init__` takes `run_id` and `git_commit`, generates timestamp once. `get(variant, rep)` returns or creates the writer. `close_all()` closes all writers. On first creation, each writer gets `write_metadata()` with model, temperature, fixed_timestamp, zero_persona, variant, rep.
- Add `rep_number` fixture that extracts 1-indexed repetition number from pytest-repeat. Use `request.config.option.count` to check if repeats are active, then `request.node.callspec.params.get("__pytest_repeat_step_number", 0) + 1`. Default to 1 when not repeating.
- Replace session-scoped `jsonl_writer` fixture with session-scoped `writer_pool` fixture.
- Add `pytest_terminal_summary` hook that prints experiment summary: passed/failed/rerun counts and total collected.

Modify `tests/experiment/test_experiment.py`:
- Change `jsonl_writer` parameter to `writer_pool` and add `rep_number` parameter.
- Set `temperature=EVAL_TEMPERATURE` on `ChatAnthropic` constructor.
- Import `EVAL_TEMPERATURE` from conftest.
- Add `rep` to `t.log_inputs()` dict.
- Add `t.log_feedback(key="rep", value=rep_number)` call.
- Replace `jsonl_writer.write_result(...)` with `writer = writer_pool.get(variant_name, rep_number)` then `writer.write_result(...)` — also add `rep=rep_number` to the write_result kwargs.

Verify the test collection works: `uv run pytest tests/experiment/test_experiment.py -m experiment --count 3 --collect-only`
Expected: 120 items collected (2 variants x 20 scenarios x 3 reps).
  </action>
  <verify>
`uv run pytest tests/experiment/test_experiment.py -m experiment --count 3 --collect-only` shows exactly 120 items.
`ruff check tests/experiment/` passes clean.
  </verify>
  <done>
capture.py accepts optional filename, conftest.py has WriterPool + rep_number + EVAL_TEMPERATURE + summary hook, test_experiment.py uses writer_pool/rep_number/temperature, pytest-rerunfailures is installed, 120 items collected with --count 3.
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute full experiment and validate completeness</name>
  <files>
    results/*.jsonl
  </files>
  <action>
Run the full experiment:
```bash
uv run pytest tests/experiment/test_experiment.py -m experiment --count 3 --reruns 2 --reruns-delay 5 -v
```

This executes 120 test items (2 variants x 20 scenarios x 3 runs). Expected wall-clock: 2-6 minutes. Cost: ~$0.12.

If rate limiting occurs (429 errors surviving reruns), add a small delay between tests by inserting `await asyncio.sleep(0.5)` in the test function body before the LLM call, then rerun.

After execution, validate completeness:
1. Check that 6 JSONL files exist in results/ (baseline_run1_*.jsonl, baseline_run2_*.jsonl, baseline_run3_*.jsonl, applike_run1_*.jsonl, applike_run2_*.jsonl, applike_run3_*.jsonl)
2. Each file should have 1 metadata line + 20 scenario_result lines = 21 lines
3. Verify no scenario_result lines have empty response_text (serialization bug is fixed)
4. Verify all 20 scenario IDs appear in each file
5. Check pytest output shows 120 passed (or 120 passed with some reruns)

Quick validation script (inline bash):
```bash
for f in results/*.jsonl; do
  echo "$f: $(wc -l < $f) lines"
  # Check for empty response_text
  python3 -c "
import json, sys
with open('$f') as fh:
    for line in fh:
        r = json.loads(line)
        if r['type'] == 'scenario_result' and not r.get('response_text'):
            print(f'EMPTY response_text: {r[\"scenario_id\"]}', file=sys.stderr)
            sys.exit(1)
print('OK')
"
done
```

If any scenarios failed all retries, rerun just the failures: `uv run pytest tests/experiment/test_experiment.py -m experiment --count 3 --reruns 2 --reruns-delay 5 -k "scenario_name" -v`
  </action>
  <verify>
6 JSONL files exist in results/ directory.
Each file has 21 lines (1 metadata + 20 results).
All 120 test items passed (check pytest exit code 0).
No empty response_text in any scenario_result line.
  </verify>
  <done>
120 LLM calls complete across 3 runs x 2 variants. 6 JSONL files in results/ with full data. LangSmith traces annotated with variant, run_id, and rep number. All scenarios covered. Data ready for Phase 10 review.
  </done>
</task>

</tasks>

<verification>
1. `ls results/*.jsonl | wc -l` returns 6
2. Each JSONL file has 21 lines (1 metadata + 20 results)
3. All scenario IDs present in each variant's files
4. No empty response_text fields
5. pytest reported 120 passed
6. `uv run pytest tests/experiment/test_experiment.py -m experiment --count 3 --collect-only` shows 120 items
</verification>

<success_criteria>
- 6 JSONL files (2 variants x 3 runs) exist with complete data
- All 120 LLM calls produced non-empty responses with tool call metadata
- LangSmith traces include rep number annotations for filtering
- Temperature set to 0.2 across all calls
- Partial rerun capability verified (timestamped files don't overwrite)
</success_criteria>

<output>
After completion, create `.planning/phases/09-run-experiments/09-01-SUMMARY.md`
</output>
