---
phase: 07-infrastructure-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/eval/test_tasks.py
  - tests/eval/cache/baseline/single_reminder.json
  - tests/eval/cache/rename/
  - tests/eval/cache/simplify/
  - tests/eval/reports/latest.json
  - eval_results.txt
  - eval_results_v2.txt
  - eval_results_v3.txt
  - eval_results_v4.txt
  - eval_results_v5.txt
  - eval_results_phase4.txt
  - eval_phase5_initial.txt
  - eval_phase5_explore_1.txt
  - eval_phase5_explore_2.txt
autonomous: true
requirements:
  - INFRA-01
  - INFRA-02

must_haves:
  truths:
    - "AIMessage with list-type content (text + tool_use dicts) serializes to non-empty content string"
    - "AIMessage with plain string content still serializes correctly (no regression)"
    - "No v1.0 cached responses exist -- cache directory contains only .gitkeep"
    - "No v1.0 eval result files exist at project root"
  artifacts:
    - path: "tests/eval/test_tasks.py"
      provides: "Fixed _serialize_response that handles list-type AIMessage.content"
      contains: "isinstance(content, list)"
    - path: "tests/eval/cache/.gitkeep"
      provides: "Empty cache directory preserved for git tracking"
  key_links:
    - from: "tests/eval/test_tasks.py:_serialize_response"
      to: "tests/eval/test_tasks.py:_deserialize_response"
      via: "round-trip: serialize stores extracted text string, deserialize expects string"
      pattern: "content.*isinstance.*list"
---

<objective>
Fix the two known data-integrity bugs in the eval pipeline so the measurement instrument produces correct, complete data.

Purpose: INFRA-01 (serialization discards list-type content) and INFRA-02 (corrupted v1.0 cache) are blocking trustworthy experiment results. Without these fixes, re-running experiments would produce the same corrupted data.

Output: Fixed `_serialize_response()`, empty cache, no stale eval artifacts, verified round-trip.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@tests/eval/test_tasks.py
@scripts/eval_probe.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix _serialize_response to handle list-type AIMessage.content</name>
  <files>tests/eval/test_tasks.py</files>
  <action>
In `_serialize_response()` (line 27), replace the content extraction logic:

**Current (buggy):**
```python
"content": response.content if isinstance(response.content, str) else "",
```

**Replace with:**
```python
content = response.content
if isinstance(content, list):
    text_parts = [c["text"] for c in content if isinstance(c, dict) and c.get("type") == "text"]
    content = " ".join(text_parts)
```
Then use the extracted `content` variable in the return dict.

This matches the proven pattern from `scripts/eval_probe.py:42-44`.

`_deserialize_response` requires NO changes -- it already expects a string content field, and we're now storing the extracted text string (same type, just non-empty).

Also add response text to `t.log_outputs` in `test_positive` (line 108) so LangSmith captures full response for review:
```python
t.log_outputs({"response_text": <extracted_text>, "tool_call_names": result.tool_call_names, "call_count": result.call_count})
```
To get the text, extract it from `response.content` using the same list-handling pattern before the `t.log_outputs` call.

Add a unit test function `test_serialize_deserialize_roundtrip` at the bottom of the file that:
1. Creates an AIMessage with list-type content: `[{"type": "text", "text": "I'll set that reminder."}, {"type": "tool_use", "id": "toolu_123", "name": "schedule_task", "input": {}}]` and `tool_calls=[{"name": "schedule_task", "args": {"task": "test"}, "id": "toolu_123"}]`
2. Calls `_serialize_response` and asserts `result["content"] == "I'll set that reminder."`
3. Calls `_deserialize_response` on the result and asserts the round-trip content matches
4. Also tests with plain string content to verify no regression
  </action>
  <verify>
Run `uv run pytest tests/eval/test_tasks.py::test_serialize_deserialize_roundtrip -v` -- all assertions pass.
Run `uv run ruff check tests/eval/test_tasks.py` -- no lint errors.
  </verify>
  <done>
`_serialize_response` correctly extracts text from list-type AIMessage.content (non-empty string in output). Round-trip test passes for both list and string content types. No lint errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wipe corrupted v1.0 cache and stale eval artifacts</name>
  <files>
tests/eval/cache/baseline/single_reminder.json
tests/eval/cache/rename/*.json
tests/eval/cache/simplify/*.json
tests/eval/reports/latest.json
tests/eval/reports/phase4_summary.md
eval_results.txt
eval_results_v2.txt
eval_results_v3.txt
eval_results_v4.txt
eval_results_v5.txt
eval_results_phase4.txt
eval_phase5_initial.txt
eval_phase5_explore_1.txt
eval_phase5_explore_2.txt
  </files>
  <action>
Delete all corrupted v1.0 data per locked decision (wipe entire cache, delete old eval result files, clean slate):

1. Delete all JSON files inside `tests/eval/cache/` subdirectories:
   ```bash
   rm -f tests/eval/cache/baseline/*.json
   rm -f tests/eval/cache/rename/*.json
   rm -f tests/eval/cache/simplify/*.json
   ```
   Preserve `tests/eval/cache/.gitkeep`.

2. Remove empty variant subdirectories (they'll be recreated on next cache write):
   ```bash
   rmdir tests/eval/cache/baseline tests/eval/cache/rename tests/eval/cache/simplify
   ```

3. Delete stale reports:
   ```bash
   rm -f tests/eval/reports/latest.json tests/eval/reports/phase4_summary.md
   ```

4. Delete root-level eval result files:
   ```bash
   rm -f eval_results.txt eval_results_v2.txt eval_results_v3.txt eval_results_v4.txt eval_results_v5.txt
   rm -f eval_results_phase4.txt eval_phase5_initial.txt eval_phase5_explore_1.txt eval_phase5_explore_2.txt
   ```

v1.0 milestone docs in `.planning/milestones/` are NOT touched (those are project docs, not eval data).
  </action>
  <verify>
```bash
# Cache dir has only .gitkeep
ls tests/eval/cache/
# Should show only .gitkeep

# No root-level eval files
ls eval_*.txt 2>/dev/null
# Should show nothing

# Reports dir empty or has only .gitkeep
ls tests/eval/reports/
# Should be empty or show nothing
```
  </verify>
  <done>
All v1.0 cached responses deleted. All root-level eval result files deleted. All stale reports deleted. `tests/eval/cache/.gitkeep` preserved. Clean slate for fresh experiment runs.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/eval/test_tasks.py::test_serialize_deserialize_roundtrip -v` passes -- serialization bug is fixed and verified via round-trip
2. `ls tests/eval/cache/` shows only `.gitkeep` -- no corrupted cache files remain
3. `ls eval_*.txt 2>/dev/null` returns nothing -- no stale eval artifacts at root
4. `uv run ruff check tests/eval/test_tasks.py` -- no lint errors
5. Phase 7 success criteria from ROADMAP are met:
   - Running a scenario with list-type content would produce JSONL with non-empty response text
   - No v1.0 cached responses can contaminate results
</verification>

<success_criteria>
- _serialize_response handles both string and list-type AIMessage.content, producing non-empty text
- Round-trip unit test passes (serialize -> deserialize preserves content)
- Zero cached JSON files in tests/eval/cache/
- Zero eval_*.txt files at project root
- Zero stale reports in tests/eval/reports/
- No ruff lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/07-infrastructure-fixes/07-01-SUMMARY.md`
</output>
