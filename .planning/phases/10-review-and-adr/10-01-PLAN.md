---
phase: 10-review-and-adr
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/analyze_experiment.py
  - docs/adr-tool-interface-experiment.md
autonomous: true
requirements: [ANLS-02, ANLS-03]

must_haves:
  truths:
    - "Every failure/poor transcript has been read and annotated by a human reviewer — no conclusions drawn from aggregate stats alone"
    - "Blind review protocol followed: qualitative observations recorded before aggregate pass rates computed"
    - "ADR documents hypothesis, methodology, results, and a clear ADOPT/REJECT/REVISIT decision"
    - "v1.1 results compared against v1.0 findings with explanation of discrepancies"
  artifacts:
    - path: "scripts/analyze_experiment.py"
      provides: "Reproducible analysis script that reads JSONL, applies rubric scores, computes stats"
      contains: "bootstrap_ci"
    - path: "docs/adr-tool-interface-experiment.md"
      provides: "Updated ADR with v1.1 data, methodology, and decision"
      contains: "v1.1"
  key_links:
    - from: "scripts/analyze_experiment.py"
      to: "tests/eval/stats.py"
      via: "imports bootstrap_ci, fisher_exact_comparison"
      pattern: "from tests.eval.stats import"
    - from: "scripts/analyze_experiment.py"
      to: "results/*.jsonl"
      via: "reads JSONL files"
      pattern: "json.loads"
---

<objective>
Perform a structured blind review of all 120 experiment transcripts, compute aggregate statistics, and produce an updated ADR with a defensible ADOPT/REJECT/REVISIT decision on tool interface strategy.

Purpose: This is the capstone of v1.1 — translating raw experiment data into an evidence-based architectural decision. The blind review protocol (read transcripts before aggregate stats) prevents the confirmation bias that invalidated v1.0.

Output: Updated `docs/adr-tool-interface-experiment.md` with v1.1 conclusions, plus a reproducible analysis script.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-review-and-adr/10-RESEARCH.md
@.planning/phases/09-run-experiments/09-01-SUMMARY.md
@docs/adr-tool-interface-experiment.md
@tests/eval/stats.py
@tests/experiment/scenarios.py
@results/baseline_run1_20260220_014556.jsonl
@results/baseline_run2_20260220_014556.jsonl
@results/baseline_run3_20260220_014556.jsonl
@results/applike_run1_20260220_014556.jsonl
@results/applike_run2_20260220_014556.jsonl
@results/applike_run3_20260220_014556.jsonl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Blind review of transcripts and reproducible analysis script</name>
  <files>scripts/analyze_experiment.py</files>
  <action>
Phase A — Blind qualitative review:

1. Read ALL 120 transcripts from the 6 JSONL files (results/*.jsonl). Parse each `scenario_result` line.
2. Group results by scenario_id (20 scenarios x 6 results each = both variants x 3 runs).
3. For each scenario, review all 6 results together (3 baseline + 3 applike). Record per-result observations BEFORE computing any aggregate statistics.
4. Apply the evaluation rubric from 10-RESEARCH.md with 4 dimensions (tool_selection, parameter_quality, ambiguity_handling, naturalness), each scored as good(3)/acceptable(2)/poor(1).
5. Key rubric rules:
   - Clarification IS a valid response when the prompt is genuinely ambiguous (do NOT penalize like v1.0)
   - Tool-only responses (empty text + tool_calls) are valid for clear scheduling requests
   - For negative scenarios, conversational response with no tool calls = good
   - Cross-run consistency matters: 3/3 identical good responses > 2 great + 1 poor

Phase B — Scoring and aggregation:

6. Create `scripts/analyze_experiment.py` that:
   - Reads all 6 JSONL files from results/
   - Encodes the per-scenario rubric scores from Phase A as a dict literal in the script (human-in-the-loop: scores were determined by reading transcripts)
   - Reduces 4-dimension scores to binary pass/fail: any dimension scored "poor" (1) = fail (0.0), otherwise pass (1.0)
   - Computes per-variant, per-category, and overall statistics using `tests.eval.stats.bootstrap_ci()` and `tests.eval.stats.fisher_exact_comparison()`
   - Prints results tables to stdout for embedding in the ADR
   - Uses loguru for progress logging per project standards

7. Run the analysis script and capture output.

Phase C — Record key qualitative findings:

8. Identify the 3-5 most informative transcript pairs (same scenario, different variant behavior) that illustrate the core patterns. These will be quoted in the ADR.
9. Focus analytical attention on ambiguous (6 scenarios, 36 results) and routing (4 scenarios, 24 results) categories — these are where signal lives per 10-RESEARCH.md.

IMPORTANT: The research phase already read all 120 transcripts and identified key patterns. Use those observations as a starting point but verify against the actual JSONL data. Key findings from research:
- Applike tends to act with assumed defaults on ambiguous scenarios (vague_timing: 15min, wake_up: 8am, after_work: 5pm)
- Baseline tends to ask clarification on ambiguous scenarios
- This is INVERTED from v1.0 — suggests v1.0 pattern was a persona artifact
- Sanity, routing, negative categories show near-identical behavior between variants
  </action>
  <verify>
  - `uv run python scripts/analyze_experiment.py` runs without error and prints aggregate statistics
  - `ruff check scripts/analyze_experiment.py` passes
  - Output includes per-category bootstrap CIs and Fisher exact p-values for baseline vs applike
  - Script reads all 6 JSONL files and processes exactly 120 scenario results
  </verify>
  <done>
  - Rubric scores assigned to all 120 transcripts after blind reading
  - Reproducible analysis script exists and produces aggregate tables
  - Per-category and overall pass rates with CIs computed for both variants
  - Fisher exact tests computed for baseline vs applike overall and per key category
  - 3-5 illustrative transcript pairs identified for ADR
  </done>
</task>

<task type="auto">
  <name>Task 2: Produce updated ADR with v1.1 conclusions</name>
  <files>docs/adr-tool-interface-experiment.md</files>
  <action>
Replace the existing v1.0 ADR at `docs/adr-tool-interface-experiment.md` with a v1.1 ADR that incorporates the clean experiment data. Follow the structure from 10-RESEARCH.md Pattern 4.

ADR sections:

1. **Status** — DECIDED with the decision (ADOPT/REJECT/REVISIT)

2. **Problem Statement** — Reuse the core question from v1.0 ADR (same problem). Update context to reflect v1.1 methodology changes.

3. **Hypothesis** — Same as v1.0: app-like tool decomposition (Calendar/Reminders) would improve routing accuracy by matching user mental models.

4. **Methodology** — Document v1.1 experiment design:
   - 20 scenarios across 5 categories (sanity/ambiguous/routing/negative/implicit)
   - Zero-persona prompt (no Joi persona — tool interface as only variable)
   - Fixed timestamp (2026-02-15 10:00 UTC, Saturday)
   - 3 repetitions at temperature 0.2
   - 120 total LLM calls
   - Blind human review protocol (transcripts before stats)
   - 4-dimension rubric (tool selection, parameter quality, ambiguity handling, naturalness)
   - Include table comparing v1.0 vs v1.1 methodology differences (from 10-RESEARCH.md Pattern 3)

5. **Results** — Embed the aggregate statistics from Task 1's analysis script output:
   - Overall pass rates with 95% CIs for both variants
   - Per-category breakdown table
   - Fisher exact test results for key comparisons
   - Token usage comparison

6. **Qualitative Findings** — The "why" behind the numbers:
   - Document the decisiveness-vs-clarification pattern discovered in v1.1
   - Include 3-5 illustrative transcript excerpts (scenario prompt + both variant responses)
   - Explain the v1.0 inversion: what changed and why (zero-persona eliminates tool name bias)
   - Note cross-run stability patterns

7. **Decision** — State the decision clearly:
   - Use the decision framework from 10-RESEARCH.md: ADOPT requires clear positive signal + significance; REJECT if no benefit or negative signal (Occam's razor: simpler wins); REVISIT if contradictory evidence
   - Ground the decision in both quantitative and qualitative evidence

8. **Why (Root Cause Analysis)** — Explain the observed patterns:
   - Why does the tool interface affect ambiguity handling?
   - What does this tell us about how Claude Haiku 4.5 uses tool definitions?
   - Compare to v1.0 root cause analysis

9. **Consequences** — What the decision means:
   - For current Joi development (tool design principles)
   - For future tool expansion (15-20+ tools)
   - What the next milestone should focus on

10. **Limitations** — What the experiment cannot conclude:
    - Model-specific (Haiku 4.5 only)
    - Domain-specific (task scheduling only)
    - n=3 per scenario power limitations
    - Zero-persona vs real persona differences

11. **v1.0 Comparison** — Explicit section:
    - What v1.0 found vs what v1.1 found
    - Why they differ (methodological improvements)
    - Which conclusion is more trustworthy and why

Keep the ADR under 300 lines. It's a decision document, not a paper.
  </action>
  <verify>
  - `docs/adr-tool-interface-experiment.md` exists and contains "v1.1"
  - ADR has a clear Status line with ADOPT/REJECT/REVISIT
  - ADR has Methodology section documenting v1.1 zero-persona experiment
  - ADR has Results section with statistical tables from the analysis script
  - ADR has Qualitative Findings section with transcript excerpts
  - ADR has v1.0 Comparison section explaining discrepancies
  - ADR is under 300 lines
  </verify>
  <done>
  - ADR documents hypothesis, v1.1 methodology, results from clean data
  - ADR contains a clear ADOPT/REJECT/REVISIT decision grounded in evidence
  - ADR includes both quantitative stats and qualitative transcript examples
  - ADR compares v1.0 and v1.1 findings with explanation of differences
  - ADR states limitations and scope of the decision
  - ADR replaces the v1.0 ADR in place
  </done>
</task>

</tasks>

<verification>
1. `uv run python scripts/analyze_experiment.py` completes successfully with statistics output
2. `ruff check scripts/analyze_experiment.py` passes
3. `docs/adr-tool-interface-experiment.md` contains "v1.1" and a clear decision
4. ADR is under 300 lines: `wc -l docs/adr-tool-interface-experiment.md`
5. ADR contains all required sections: Status, Problem Statement, Hypothesis, Methodology, Results, Qualitative Findings, Decision, Why, Consequences, Limitations, v1.0 Comparison
</verification>

<success_criteria>
- Blind review protocol demonstrably followed (qualitative observations recorded before aggregate stats computed)
- Every poor/failure transcript accounted for in the ADR's qualitative findings or analysis
- Analysis script is reproducible (same output on re-run)
- ADR makes a clear, defensible decision (not "we need more data")
- Decision grounded in both numbers (stats) and judgment (transcript review)
- v1.0 vs v1.1 discrepancies explained
</success_criteria>

<output>
After completion, create `.planning/phases/10-review-and-adr/10-01-SUMMARY.md`
</output>
