---
phase: 08-experiment-harness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/experiment/conftest.py
  - tests/experiment/capture.py
  - tests/experiment/variants/registry.py
  - tests/experiment/variants/baseline.py
  - tests/experiment/variants/applike.py
  - tests/experiment/parity.py
  - pyproject.toml
autonomous: true
requirements:
  - EXPR-01
  - EXPR-02
  - EXPR-03
  - CAPT-01
  - CAPT-02

must_haves:
  truths:
    - "Zero-persona system prompt exists as a constant with no Joi personality, no tool name references"
    - "Both tool variants (baseline, applike) are registered and produce working tool lists via tools_factory()"
    - "JSONL writer produces valid JSONL with run_metadata line and scenario_result lines"
    - "Parity tests pass confirming both variants can schedule one-time, recurring, list, and update"
    - "Fixed timestamp constant exists and is used for all scenario invocations (no datetime.now())"
    - "pytest 'experiment' marker is registered and usable"
  artifacts:
    - path: "tests/experiment/conftest.py"
      provides: "Fixtures for experiment mode: run_id, jsonl_writer, fixed timestamp constant, zero-persona constant"
    - path: "tests/experiment/capture.py"
      provides: "JSONLWriter class with write_metadata() and write_result()"
    - path: "tests/experiment/variants/registry.py"
      provides: "Simplified ToolVariant dataclass (no persona field) and VARIANTS dict"
    - path: "tests/experiment/variants/baseline.py"
      provides: "Baseline variant with schedule_task, list_tasks, update_task tools"
    - path: "tests/experiment/variants/applike.py"
      provides: "Applike variant with calendar_create_event, reminders_create, calendar_list_events, calendar_update_event tools"
    - path: "tests/experiment/parity.py"
      provides: "Static parity tests verifying both variants cover all required capabilities"
  key_links:
    - from: "tests/experiment/conftest.py"
      to: "tests/experiment/capture.py"
      via: "jsonl_writer fixture creates JSONLWriter"
      pattern: "JSONLWriter"
    - from: "tests/experiment/variants/registry.py"
      to: "tests/experiment/variants/baseline.py"
      via: "auto-import triggers registration"
      pattern: "import.*baseline"
    - from: "tests/experiment/parity.py"
      to: "tests/experiment/variants/registry.py"
      via: "VARIANTS dict import for schema inspection"
      pattern: "from.*registry import VARIANTS"
---

<objective>
Build the experiment infrastructure: simplified tool variants (no persona), JSONL capture, parity checks, and shared fixtures. This is the foundation for experiment execution in plan 02.

Purpose: Create all building blocks that the experiment test needs — variants, capture, fixtures, parity verification — so plan 02 can focus purely on scenario design and the test itself.
Output: `tests/experiment/` directory with working variants, capture module, parity tests, and conftest.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-experiment-harness/08-RESEARCH.md

# Existing variant implementations (reference for tool definitions — re-implement, do NOT import)
@tests/eval/variants/tasks_baseline.py
@tests/eval/variants/tasks_applike.py
@tests/eval/variants/registry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create experiment infrastructure — variants, capture, and conftest</name>
  <files>
    tests/experiment/conftest.py
    tests/experiment/capture.py
    tests/experiment/variants/registry.py
    tests/experiment/variants/baseline.py
    tests/experiment/variants/applike.py
    pyproject.toml
  </files>
  <action>
Create `tests/experiment/` directory tree:
```
tests/experiment/
├── conftest.py
├── capture.py
└── variants/
    ├── registry.py
    ├── baseline.py
    └── applike.py
```

**`tests/experiment/variants/registry.py`** — Simplified ToolVariant dataclass:
```python
@dataclass
class ToolVariant:
    name: str
    tools_factory: Callable[[], list[BaseTool]]
    schedule_tool_name: str = "schedule_task"
    schedule_tool_names: list[str] | None = None
    description: str = ""
```
NO `persona` field. NO `schedule_action` field. Keep the `register()` decorator pattern from v1.0 but simplified. VARIANTS dict populated via auto-imports at bottom of file.

**`tests/experiment/variants/baseline.py`** — Re-implement from `tests/eval/variants/tasks_baseline.py`:
- Same tool definitions: `schedule_task` (5 params: title, description, when, delay_seconds, recurring), `list_tasks`, `update_task`
- Remove `run_code` tool (orthogonal to scheduling experiment)
- NO persona reading — variant has no persona field
- Register as "baseline"

**`tests/experiment/variants/applike.py`** — Re-implement from `tests/eval/variants/tasks_applike.py`:
- Same tool definitions: `calendar_create_event` (title, description, when), `reminders_create` (title, description, schedule), `calendar_list_events`, `calendar_update_event`
- NO persona reading, NO `_patch_persona` function
- Register as "applike"

**`tests/experiment/capture.py`** — JSONLWriter class:
- Constructor takes `run_id: str`, `git_commit: str`
- Creates `results/` directory, opens file `results/experiment_{run_id}_{ts}.jsonl`
- `write_metadata(**kwargs)` — writes `{"type": "run_metadata", "run_id": ..., "git_commit": ..., "timestamp": now_utc, ...kwargs}` + flush
- `write_result(**kwargs)` — writes `{"type": "scenario_result", "run_id": ..., ...kwargs}` + flush
- `close()` — closes file handle
- The `timestamp` in metadata is the wall-clock time of the run (OK to use datetime.now for metadata timestamp — this is run metadata, not scenario timing). The FIXED_TIMESTAMP is for scenario content only.

**`tests/experiment/conftest.py`** — Shared fixtures:
- `EVAL_MODEL = "claude-haiku-4-5-20251001"` — constant
- `FIXED_TIMESTAMP = "2026-02-15 10:00 UTC"` — constant (Saturday morning)
- `ZERO_PERSONA` — constant: "You are a task scheduling assistant. Use the available tools to handle the user's request. If the request is about scheduling, reminders, or timed actions, use the scheduling tools. If the request is not about scheduling, respond conversationally without using tools."
- `run_id` fixture (session scope) — returns `uuid4().hex[:12]`
- `jsonl_writer` fixture (session scope) — creates JSONLWriter, calls write_metadata with model, fixed_timestamp, zero_persona, variant descriptions from VARIANTS dict. Yields writer, calls close() after.
- NO `datetime.now` in fixtures except for the run metadata timestamp in JSONLWriter (which is the wall-clock of the run itself, not injected into scenarios).

**`pyproject.toml`** — Add `experiment` marker:
Add to the `markers` list: `"experiment: marks tests as experiment harness tests (hits real API, captures JSONL)"`

Add `results/` to .gitignore if not already there.
  </action>
  <verify>
```bash
# Verify files exist
ls tests/experiment/conftest.py tests/experiment/capture.py tests/experiment/variants/registry.py tests/experiment/variants/baseline.py tests/experiment/variants/applike.py

# Verify imports work
uv run python -c "from tests.experiment.variants.registry import VARIANTS; print(list(VARIANTS.keys()))"

# Should print: ['baseline', 'applike']

# Verify capture module works
uv run python -c "
from tests.experiment.capture import JSONLWriter
w = JSONLWriter('test123', 'abc')
w.write_metadata(model='test')
w.close()
import json
from pathlib import Path
f = list(Path('results').glob('experiment_test123_*.jsonl'))[0]
line = json.loads(f.read_text().strip())
assert line['type'] == 'run_metadata'
print('JSONL capture works')
f.unlink()
"

# Verify marker registered
uv run pytest --markers | grep experiment

# Lint
uv run ruff check tests/experiment/
```
  </verify>
  <done>
    - VARIANTS dict has exactly "baseline" and "applike" entries, both with working tools_factory()
    - JSONLWriter produces valid JSONL with run_metadata type
    - Zero-persona constant contains no tool names, no personality
    - Fixed timestamp constant is "2026-02-15 10:00 UTC"
    - "experiment" marker registered in pyproject.toml
    - No imports from tests/eval/ — completely independent
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement tool parity checks and clean up v1.0 eval code</name>
  <files>
    tests/experiment/parity.py
    tests/eval/conftest.py
    tests/eval/evaluators.py
    tests/eval/test_tasks.py
    tests/eval/token_budget.py
    tests/eval/scenarios/
    tests/eval/variants/
    tests/eval/cache/
    scripts/eval_probe.py
  </files>
  <action>
**`tests/experiment/parity.py`** — Static parity tests. Mark all with `@pytest.mark.experiment`. Tests verify at schema level (no LLM calls):

1. `test_both_variants_can_schedule_one_time()` — For each variant, check that at least one tool in schedule_tool_names (or schedule_tool_name) exists in the tools list.

2. `test_both_variants_can_create_recurring()` — For baseline: schedule_task has `recurring` param. For applike: reminders_create tool exists with `schedule` param.

3. `test_both_variants_can_list()` — Each variant has a tool with "list" in its name.

4. `test_both_variants_can_update()` — Each variant has a tool with "update" in its name.

5. `test_scheduling_tools_have_required_params()` — For each variant's scheduling tool(s): verify they accept title, description, and some form of timing parameter (when, delay_seconds, or schedule).

6. `test_no_variant_uses_persona()` — Verify the ToolVariant dataclass has no `persona` attribute. Confirm zero-persona isolation.

7. `test_variant_count()` — Exactly 2 variants: baseline and applike.

Import VARIANTS from `tests.experiment.variants.registry`. Use `tool.args_schema.model_json_schema()` to inspect parameter schemas.

**v1.0 Cleanup** — Delete old eval infrastructure per locked decision "old tests/eval artifacts can be removed without hesitation":
- Delete `tests/eval/conftest.py`
- Delete `tests/eval/evaluators.py`
- Delete `tests/eval/test_tasks.py`
- Delete `tests/eval/token_budget.py`
- Delete `tests/eval/scenarios/` directory entirely
- Delete `tests/eval/variants/` directory entirely
- Delete `tests/eval/cache/` directory entirely
- Delete `tests/eval/reports/` directory if exists
- Delete `tests/eval/parity_matrix.md`
- Delete `scripts/eval_probe.py`
- **KEEP** `tests/eval/stats.py` — standalone stats utility, reusable for Phase 9-10

After deletion, `tests/eval/` should contain only `stats.py` (and `__pycache__` if present).
  </action>
  <verify>
```bash
# Run parity tests
uv run pytest tests/experiment/parity.py -v -m experiment

# All 7 tests should pass

# Verify v1.0 cleanup
ls tests/eval/
# Should show only: stats.py (and possibly __pycache__)

# Verify no broken imports remain
uv run python -c "from tests.eval.stats import bootstrap_ci; print('stats.py still works')"

# Verify scripts/eval_probe.py is gone
test ! -f scripts/eval_probe.py && echo "eval_probe.py deleted"

# Lint
uv run ruff check tests/experiment/parity.py
```
  </verify>
  <done>
    - 7 parity tests pass confirming both variants cover: one-time scheduling, recurring, list, update, required params, no persona, correct variant count
    - tests/eval/ contains only stats.py
    - scripts/eval_probe.py deleted
    - All v1.0 scenario YAML files deleted
    - All v1.0 variant files deleted
    - No imports from tests.eval in new experiment code
  </done>
</task>

</tasks>

<verification>
```bash
# Full verification
uv run pytest tests/experiment/parity.py -v -m experiment
uv run python -c "from tests.experiment.variants.registry import VARIANTS; assert set(VARIANTS.keys()) == {'baseline', 'applike'}"
uv run python -c "from tests.experiment.conftest import ZERO_PERSONA, FIXED_TIMESTAMP; assert 'schedule_task' not in ZERO_PERSONA; assert 'calendar' not in ZERO_PERSONA"
uv run ruff check tests/experiment/
ls tests/eval/  # Only stats.py
```
</verification>

<success_criteria>
- `tests/experiment/` directory exists with all infrastructure files
- Both variants register and produce tool lists
- Parity tests pass (7/7)
- JSONL capture produces valid output
- Zero-persona prompt contains no tool-specific names
- v1.0 eval code cleaned up (only stats.py remains)
- No lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/08-experiment-harness/08-01-SUMMARY.md`
</output>
