---
phase: 08-experiment-harness
plan: 02
type: execute
wave: 2
depends_on:
  - "08-01"
files_modified:
  - tests/experiment/scenarios.py
  - tests/experiment/test_experiment.py
autonomous: true
requirements:
  - EXPR-01
  - EXPR-03
  - EXPR-04
  - CAPT-01
  - CAPT-02

must_haves:
  truths:
    - "Each scenario is self-contained — no external context, no references to prior conversation, tests one thing"
    - "Scenarios cover the full difficulty spectrum: sanity (90%+), ambiguous (40-60%), routing stress (50-70%), negative (90%+ rejection), implicit (20-40%)"
    - "The experiment test invokes ChatAnthropic.bind_tools().ainvoke() directly — no full agent graph"
    - "Every scenario execution writes a line to JSONL with prompt, response text, tool calls, and token counts"
    - "LangSmith traces are annotated with variant name, scenario category, and run ID"
    - "No assertions judge response correctness — capture only, no evaluator logic"
    - "The fixed timestamp is injected into every HumanMessage, and datetime.now() is never used for scenario content"
  artifacts:
    - path: "tests/experiment/scenarios.py"
      provides: "Scenario dataclass and SCENARIOS list with 15-20 scenarios across 5 categories"
    - path: "tests/experiment/test_experiment.py"
      provides: "Parametrized experiment test: variant x scenario matrix with dual JSONL + LangSmith capture"
  key_links:
    - from: "tests/experiment/test_experiment.py"
      to: "tests/experiment/conftest.py"
      via: "fixtures: run_id, jsonl_writer, ZERO_PERSONA, FIXED_TIMESTAMP, EVAL_MODEL"
      pattern: "ZERO_PERSONA|FIXED_TIMESTAMP|jsonl_writer"
    - from: "tests/experiment/test_experiment.py"
      to: "tests/experiment/variants/registry.py"
      via: "VARIANTS dict for parametrization"
      pattern: "from.*registry import VARIANTS"
    - from: "tests/experiment/test_experiment.py"
      to: "tests/experiment/scenarios.py"
      via: "SCENARIOS list for parametrization"
      pattern: "from.*scenarios import SCENARIOS"
    - from: "tests/experiment/test_experiment.py"
      to: "tests/experiment/capture.py"
      via: "jsonl_writer fixture (from conftest) uses JSONLWriter"
      pattern: "jsonl_writer"
---

<objective>
Design clean scenarios and build the main experiment test that runs variant x scenario matrix with dual capture (JSONL + LangSmith). No evaluator logic — capture only.

Purpose: Create the actual experiment harness that produces data for Phase 9 execution and Phase 10 review. Scenarios designed from scratch learning from v1.0 failures (ceiling effects on easy, signal on ambiguous).
Output: `scenarios.py` with 15-20 targeted scenarios, `test_experiment.py` with parametrized capture-only test.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-experiment-harness/08-RESEARCH.md
@.planning/phases/08-experiment-harness/08-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Design scenario set — self-contained, difficulty-calibrated, no v1.0 reuse</name>
  <files>tests/experiment/scenarios.py</files>
  <action>
Create `tests/experiment/scenarios.py` with a frozen Scenario dataclass and the SCENARIOS list.

**Scenario dataclass:**
```python
@dataclass(frozen=True)
class Scenario:
    id: str           # "category:short_name"
    prompt: str       # The user message (no timestamp — that's injected by the test)
    category: str     # sanity | ambiguous | routing | negative | implicit
    description: str  # What this tests (for human review during Phase 10)
```

**Design 15-20 scenarios across these categories. Key design principles:**

1. **Sanity (3-4 scenarios, target 90%+ baseline pass)** — Simple, unambiguous scheduling requests. These verify tools work at all. Include: single one-time reminder with explicit time, single recurring task with explicit cron-like language, list request.

2. **Ambiguous intent (5-6 scenarios, target 40-60% baseline)** — The primary differentiator from v1.0 analysis (p=0.006 was the only real signal). Include:
   - Vague timing: "remind me about X in a bit", "soon", "later"
   - Unclear one-time vs recurring: "make sure I take my vitamins" (is this daily or once?), "help me wake up on time" (alarm = recurring?)
   - Implicit scheduling need: "I keep forgetting to water the plants" (scheduling implied but not requested)

3. **Routing stress (3-4 scenarios, target 50-70% baseline)** — Multiple items or ambiguous tool routing. Include:
   - Two one-time items in one request (applike must call calendar_create_event twice)
   - Mixed types: one-time + recurring in single request
   - Request that could plausibly use either tool type

4. **Negative (3-4 scenarios, target 90%+ correct rejection)** — Should NOT trigger scheduling tools. Include:
   - Scheduling-adjacent language without actual request: "I really should start setting alarms"
   - Past tense: "I had a meeting at 3pm yesterday"
   - Questions about time: "What time is it in Tokyo?"
   - Hard negative: "Can you remind me what we talked about?" (not a reminder — a recall request)

5. **Implicit timing (2-3 scenarios, target 20-40% baseline)** — Context-dependent timing that requires inference. Include:
   - "remind me before the weekend" (when is "before"? depends on the day — fixed timestamp is Saturday 10am)
   - "do the usual morning check" (no context for what "usual" means)

**Every scenario prompt must:**
- Be self-contained (no "the meeting" without specifying which meeting)
- Test exactly one thing (one decision boundary)
- Have no external context dependencies
- NOT reuse any v1.0 scenario prompts (clean slate per locked decision)

**Every scenario must NOT:**
- Reference prior conversation
- Assume the model knows the user's schedule
- Require tools outside the experiment set
  </action>
  <verify>
```bash
# Verify scenarios load and are well-formed
uv run python -c "
from tests.experiment.scenarios import SCENARIOS, Scenario
print(f'Total scenarios: {len(SCENARIOS)}')
categories = {}
for s in SCENARIOS:
    categories.setdefault(s.category, []).append(s.id)
    assert ':' in s.id, f'ID must be category:name, got {s.id}'
    assert s.category in ('sanity', 'ambiguous', 'routing', 'negative', 'implicit'), f'Unknown category: {s.category}'
    assert len(s.prompt) > 10, f'Prompt too short: {s.id}'
    assert len(s.description) > 5, f'Description too short: {s.id}'
for cat, ids in sorted(categories.items()):
    print(f'  {cat}: {len(ids)} scenarios')
assert len(SCENARIOS) >= 15, f'Need at least 15 scenarios, got {len(SCENARIOS)}'
assert len(set(s.id for s in SCENARIOS)) == len(SCENARIOS), 'Duplicate scenario IDs'
print('All scenarios valid')
"

# Lint
uv run ruff check tests/experiment/scenarios.py
```
  </verify>
  <done>
    - 15-20 scenarios defined across 5 categories
    - Category distribution: ~3-4 sanity, ~5-6 ambiguous, ~3-4 routing, ~3-4 negative, ~2-3 implicit
    - Every scenario has unique ID in "category:name" format
    - Every prompt is self-contained with no external dependencies
    - No v1.0 scenario prompts reused
    - No lint errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Build the experiment test — parametrized capture with JSONL + LangSmith</name>
  <files>tests/experiment/test_experiment.py</files>
  <action>
Create `tests/experiment/test_experiment.py` — the main experiment runner. This is a capture-only test: it invokes the LLM, records the response, and writes to JSONL + LangSmith. NO assertions about response correctness.

**Implementation:**

```python
# Markers: @pytest.mark.experiment, @pytest.mark.langsmith, @pytest.mark.asyncio
# Parametrize: variant_name x scenario (full cartesian product)
# IDs: variant names from VARIANTS.keys(), scenario IDs from SCENARIOS

async def test_scenario(variant_name, scenario, run_id, jsonl_writer):
    variant = VARIANTS[variant_name]
    llm = ChatAnthropic(model=EVAL_MODEL, api_key=settings.anthropic_api_key)
    model = llm.bind_tools(variant.tools_factory())

    # Log inputs to LangSmith
    t.log_inputs({
        "prompt": scenario.prompt,
        "variant": variant_name,
        "category": scenario.category,
        "run_id": run_id,
        "fixed_timestamp": FIXED_TIMESTAMP,
    })

    # Invoke with zero-persona system prompt and fixed timestamp
    response = await model.ainvoke([
        SystemMessage(content=ZERO_PERSONA),
        HumanMessage(content=f"[{FIXED_TIMESTAMP}]\n{scenario.prompt}"),
    ])

    # Extract response text (handle both str and list content)
    content = response.content
    if isinstance(content, list):
        text_parts = [c["text"] for c in content if isinstance(c, dict) and c.get("type") == "text"]
        response_text = " ".join(text_parts)
    else:
        response_text = content or ""

    tool_calls = [{"name": tc["name"], "args": tc["args"]} for tc in response.tool_calls]
    usage = response.usage_metadata or {}

    # Log outputs and feedback to LangSmith
    t.log_outputs({"response_text": response_text, "tool_calls": tool_calls})
    t.log_feedback(key="variant", value=variant_name)
    t.log_feedback(key="run_id", value=run_id)
    t.log_feedback(key="category", value=scenario.category)
    t.log_feedback(key="input_tokens", value=usage.get("input_tokens", 0))
    t.log_feedback(key="output_tokens", value=usage.get("output_tokens", 0))

    # Write to JSONL
    jsonl_writer.write_result(
        variant=variant_name,
        scenario_id=scenario.id,
        category=scenario.category,
        prompt=scenario.prompt,
        response_text=response_text,
        tool_calls=tool_calls,
        input_tokens=usage.get("input_tokens", 0),
        output_tokens=usage.get("output_tokens", 0),
        total_tokens=usage.get("total_tokens", 0),
    )
    # NO assertions — capture only. Evaluation happens in Phase 10.
```

**Critical constraints:**
- Import ZERO_PERSONA, FIXED_TIMESTAMP, EVAL_MODEL from conftest (they're module-level constants, importable directly)
- Import VARIANTS from `tests.experiment.variants.registry`
- Import SCENARIOS from `tests.experiment.scenarios`
- Import settings from `joi_agent_langgraph2.config` for API key
- Use `langsmith.testing as t` for log_inputs/log_outputs/log_feedback
- NEVER use `datetime.now()` anywhere in this file
- NEVER add assert statements about response quality/correctness
- The only imports needed from langchain: ChatAnthropic, HumanMessage, SystemMessage

**Verify the test is discoverable by pytest without running it (no API calls):**
```bash
uv run pytest tests/experiment/test_experiment.py --collect-only -q
```
This should show the parametrized test matrix (variant x scenario).

**grep for datetime.now in the entire tests/experiment/ directory — must return nothing.**
  </action>
  <verify>
```bash
# Collect tests without running (no API calls)
uv run pytest tests/experiment/test_experiment.py --collect-only -q

# Should show ~30-40 test items (2 variants x 15-20 scenarios)

# Verify no datetime.now in experiment code
grep -r "datetime.now" tests/experiment/ && echo "FAIL: datetime.now found" || echo "OK: no datetime.now in scenarios/tests"

# Verify no assert statements about response quality
grep -n "assert.*response\|assert.*tool_call\|assert.*pass\|assert.*score" tests/experiment/test_experiment.py && echo "FAIL: evaluator logic found" || echo "OK: capture only"

# Verify zero-persona has no tool names
uv run python -c "
from tests.experiment.conftest import ZERO_PERSONA
for name in ['schedule_task', 'calendar_create_event', 'reminders_create', 'list_tasks', 'update_task']:
    assert name not in ZERO_PERSONA, f'Tool name {name} found in zero-persona prompt!'
print('Zero-persona prompt is clean')
"

# Lint
uv run ruff check tests/experiment/test_experiment.py
```
  </verify>
  <done>
    - test_experiment.py collects 30-40 parametrized test items (2 variants x 15-20 scenarios)
    - Zero datetime.now() calls anywhere in tests/experiment/
    - Zero evaluator assertions (no assert about response quality)
    - Zero-persona prompt contains no tool-specific names
    - LangSmith annotation includes variant, category, run_id, token counts
    - JSONL write_result called with all required fields per CAPT-01
    - File passes ruff check
  </done>
</task>

</tasks>

<verification>
```bash
# Full Phase 8 verification suite

# 1. Parity tests pass
uv run pytest tests/experiment/parity.py -v -m experiment

# 2. Test collection works (no API calls)
uv run pytest tests/experiment/test_experiment.py --collect-only -q

# 3. No datetime.now in experiment code
grep -r "datetime.now" tests/experiment/test_experiment.py tests/experiment/scenarios.py tests/experiment/conftest.py && exit 1 || echo "Clean"

# 4. No evaluator assertions
grep -n "assert" tests/experiment/test_experiment.py | grep -v "import" && exit 1 || echo "No assertions in test_experiment.py"

# 5. Zero-persona isolation check
uv run python -c "
from tests.experiment.conftest import ZERO_PERSONA
assert 'schedule_task' not in ZERO_PERSONA
assert 'calendar' not in ZERO_PERSONA
assert 'reminder' not in ZERO_PERSONA.lower().split('reminders_create')[0] if 'reminders_create' in ZERO_PERSONA else True
print('Zero-persona clean')
"

# 6. JSONL schema check
uv run python -c "
from tests.experiment.capture import JSONLWriter
import json
w = JSONLWriter('verify', 'abc123')
w.write_metadata(model='test', fixed_timestamp='2026-02-15 10:00 UTC', zero_persona='test', variants={})
w.write_result(variant='baseline', scenario_id='test:1', category='test', prompt='test', response_text='', tool_calls=[], input_tokens=0, output_tokens=0, total_tokens=0)
w.close()
lines = w._path.read_text().strip().split('\n')
meta = json.loads(lines[0])
result = json.loads(lines[1])
assert meta['type'] == 'run_metadata'
assert result['type'] == 'scenario_result'
assert 'run_id' in meta and 'run_id' in result
print('JSONL schema valid')
w._path.unlink()
"

# 7. Lint everything
uv run ruff check tests/experiment/

# 8. v1.0 cleanup confirmed
ls tests/eval/ | grep -v stats.py | grep -v __pycache__ && echo "FAIL: v1.0 files remain" || echo "v1.0 cleanup complete"
```
</verification>

<success_criteria>
- 15-20 scenarios defined across 5 difficulty categories
- Experiment test collects as variant x scenario matrix (~30-40 items)
- Dual capture: JSONL + LangSmith annotation in every test execution
- Zero evaluator logic — capture only
- Fixed timestamp injected, no datetime.now() in scenario/test code
- Zero-persona prompt with no tool name references
- All parity tests pass
- All files pass ruff check
</success_criteria>

<output>
After completion, create `.planning/phases/08-experiment-harness/08-02-SUMMARY.md`
</output>
