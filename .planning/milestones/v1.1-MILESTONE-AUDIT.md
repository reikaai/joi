---
milestone: v1.1
audited: 2026-02-20T06:00:00Z
status: passed
scores:
  requirements: 11/11
  phases: 4/4
  integration: 11/11
  flows: 2/2
gaps:
  requirements: []
  integration:
    - id: "INFRA-02-gitkeep"
      severity: "info"
      description: "tests/eval/cache/.gitkeep claimed in Phase 7 SUMMARY but not tracked in git; directory does not exist. Non-functional — Phase 8-10 use results/ exclusively."
      affected_requirements: ["INFRA-02"]
  flows: []
tech_debt:
  - phase: 08-experiment-harness
    items:
      - "generate_report() in tests/eval/stats.py is orphaned v1.0 legacy code — never called by v1.1 analysis pipeline"
      - "compare_variants() argument order in analyze_experiment.py inverts a/b naming convention (functionally correct)"
  - phase: 07-infrastructure-fixes
    items:
      - "tests/eval/cache/.gitkeep not tracked in git — empty directory marker missing"
---

# Milestone Audit: v1.1 Eval Pipeline Rebuild & Re-validation

**Audited:** 2026-02-20
**Status:** PASSED
**Score:** 11/11 requirements satisfied, 4/4 phases verified, 2/2 E2E flows complete

## Milestone Goal

> Get trustworthy experiment results so we can make defensible decisions about tool interfaces. Fix the 5 systemic bugs that invalidated v1.0 data, rebuild the experiment harness with proper isolation, run clean experiments, and make a decision grounded in real evidence.

**Achieved:** Yes. Clean experiment data collected (120 transcripts, 6 JSONL files), blind-reviewed with 4-dimension rubric, and ADR produced with REJECT decision grounded in both quantitative statistics and qualitative transcript analysis.

## Phase Summary

| Phase | Goal | Status | Score | Key Output |
|-------|------|--------|-------|------------|
| 7. Infrastructure Fixes | Fix serialization bug, wipe cache | PASSED | 4/4 | `_serialize_response` fix, clean cache |
| 8. Experiment Harness | Zero-persona isolated experiment mode | PASSED | 13/13 | 8 artifacts, 7 parity tests, 20 scenarios |
| 9. Run Experiments | Execute both variants, collect JSONL | PASSED | 5/5 | 6 JSONL files, 120 scenario results |
| 10. Review and ADR | Blind review, analysis, ADR decision | PASSED | 4/4 | Analysis script, ADR with REJECT decision |

## Requirements Coverage

| Requirement | Phase | Status | Evidence |
|-------------|-------|--------|----------|
| INFRA-01 | 7 | Satisfied | `_serialize_response` handles list-type content; pattern re-applied in Phase 8 harness |
| INFRA-02 | 7 | Satisfied | Cache wiped; Phase 8-10 use `results/` exclusively |
| CAPT-01 | 8 | Satisfied | JSONLWriter captures prompt, response text, tool calls, tokens, metadata |
| CAPT-02 | 8 | Satisfied | WriterPool writes run_metadata line with model, git commit, timestamp, variants |
| EXPR-01 | 8 | Satisfied | ZERO_PERSONA constant, no tool names, no Joi personality |
| EXPR-02 | 8 | Satisfied | 7 parity tests all pass |
| EXPR-03 | 8 | Satisfied | FIXED_TIMESTAMP = "2026-02-15 10:00 UTC", no datetime.now() |
| EXPR-04 | 8 | Satisfied | 20 self-contained scenarios across 5 categories |
| ANLS-01 | 9 | Satisfied | 6 JSONL files (2 variants x 3 runs x 20 scenarios = 120 results) |
| ANLS-02 | 10 | Satisfied | Blind review: RUBRIC_SCORES dict assigned before aggregation |
| ANLS-03 | 10 | Satisfied | ADR v1.1 with REJECT decision, 239 lines |

**Orphaned requirements:** None
**Coverage:** 11/11 mapped, 11/11 verified, 0 unsatisfied

## Cross-Phase Integration

| Chain | Status | Details |
|-------|--------|---------|
| Phase 7 serialization fix → Phase 8 harness | WIRED | Pattern re-applied in test_experiment.py |
| Phase 8 harness → Phase 9 JSONL output | WIRED | WriterPool, rep_number, JSONLWriter chain complete |
| Phase 9 JSONL → Phase 10 analysis | WIRED | load_results() reads all 6 files, processes 120 results |
| Phase 8 stats.py → Phase 10 analysis script | WIRED | bootstrap_ci, fisher_exact_comparison, compare_variants imported |

**E2E Flows:** 2/2 complete (Experiment Execution, Analysis & ADR)
**Broken Flows:** 0

## Tech Debt

3 items across 2 phases (all non-blocking):

1. **Phase 7:** `tests/eval/cache/.gitkeep` not tracked in git — empty directory marker from Phase 7 SUMMARY not committed. No functional impact (Phase 8-10 use `results/`).

2. **Phase 8:** `generate_report()` in `tests/eval/stats.py` is orphaned v1.0 legacy code. Never called by the v1.1 analysis pipeline. References a `correct_tool_score` field that no longer exists. Dead code, harmless.

3. **Phase 8:** `compare_variants()` argument order in `analyze_experiment.py` inverts the `a/b` naming convention. Functionally correct (confirmed by live run producing `Difference: +0.0%`), but cosmetically confusing.

## Conclusion

Milestone v1.1 achieved its definition of done. The eval pipeline was rebuilt with proper isolation (zero-persona, fixed timestamps, parity checks), clean experiments produced 120 transcripts, and blind review led to a defensible REJECT decision on app-like tool interfaces. All 11 requirements are satisfied, all cross-phase integrations are wired, and both E2E flows complete without breaks. The 3 tech debt items are informational and require no action before milestone archiving.
