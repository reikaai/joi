---
phase: 03-app-like-variant-design
plan: 02
type: execute
wave: 2
depends_on:
  - "03-01"
files_modified:
  - tests/eval/variants/tasks_applike.py
  - tests/eval/variants/registry.py
  - tests/eval/parity_matrix.md
  - tests/eval/token_budget.py
autonomous: true
requirements:
  - EXPR-01

must_haves:
  truths:
    - "App-like variant splits schedule_task into calendar_create_event (one-shot) and reminders_create (recurring)"
    - "App-like variant has a modified system prompt section framing tools as apps ('You have a Calendar app...')"
    - "Parity matrix covers every parameter and behavior of baseline across all 6 variants"
    - "Token budget measurement shows tool definition token counts for all variants"
    - "App-like tool definitions do not exceed baseline by more than 10% in token count"
    - "All 6 variants are registered and discoverable"
  artifacts:
    - path: "tests/eval/variants/tasks_applike.py"
      provides: "Full app-like variant with Calendar/Reminders decomposition"
      contains: "reminders_create"
    - path: "tests/eval/parity_matrix.md"
      provides: "Capability parity documentation across all variants"
      contains: "calendar_create_event"
    - path: "tests/eval/token_budget.py"
      provides: "Token measurement script for all variants"
      contains: "get_num_tokens_from_messages"
  key_links:
    - from: "tests/eval/variants/tasks_applike.py"
      to: "tests/eval/variants/registry.py"
      via: "@register decorator + schedule_tool_names list"
      pattern: "schedule_tool_names"
    - from: "tests/eval/token_budget.py"
      to: "tests/eval/variants/registry.py"
      via: "imports VARIANTS dict to iterate all variants"
      pattern: "from tests.eval.variants.registry import VARIANTS"
---

<objective>
Create the full app-like variant (Calendar/Reminders decomposition with modified system prompt), produce a capability parity matrix, and implement token budget measurement.

Purpose: The app-like variant is the primary experiment hypothesis. The parity matrix ensures no silent capability loss. Token budget validates the 10% efficiency constraint.
Output: App-like variant registered, parity matrix documented, token budget script runnable.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-app-like-variant-design/03-RESEARCH.md
@.planning/phases/03-app-like-variant-design/03-01-SUMMARY.md
@tests/eval/variants/registry.py
@tests/eval/variants/tasks_baseline.py
@tests/eval/evaluators.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create full app-like variant with Calendar/Reminders decomposition</name>
  <files>
tests/eval/variants/tasks_applike.py
tests/eval/variants/registry.py
  </files>
  <action>
Create the full app-like variant following RESEARCH.md specifications. This is the only variant that changes MULTIPLE dimensions (names + params + descriptions + system prompt).

**Tool decomposition (4 tools, no run_code):**
1. `calendar_create_event(title, description, when)` — one-shot events. `when` accepts ISO datetime or delay (seconds as int or "5 minutes"). No recurring/delay_seconds params.
2. `reminders_create(title, description, schedule)` — recurring tasks. `schedule` is a cron expression. Dedicated tool for recurring, no boolean flag.
3. `calendar_list_events(status_filter=None)` — lists all events and reminders.
4. `calendar_update_event(event_id, action, detail="")` — update status. Actions: cancel, complete, fail, retry, progress. Merges task_id->event_id, drops retry_in/question/message params (all go through detail).

Use descriptions from RESEARCH.md code examples. Descriptions should be app-contextual ("Create a one-time calendar event" not "Schedule ONE background task").

**System prompt:** Read `settings.persona_path.read_text()` then replace the "Background Tasks" section with an app-oriented framing. Create a helper function that patches the persona string, replacing tool references with app-style language:
- "You have a Calendar app for scheduling events and a Reminders app for recurring tasks"
- Replace any explicit tool name references (schedule_task, list_tasks, update_task) with the new names
- Keep the rest of the persona identical

**Registration:**
- `@register("applike")`
- `schedule_tool_name="calendar_create_event"` (primary)
- `schedule_tool_names=["calendar_create_event", "reminders_create"]` (both scheduling tools)

Add auto-import to registry.py:
```python
import tests.eval.variants.tasks_applike  # noqa: E402, F401
```
  </action>
  <verify>
```bash
uv run python -c "
from tests.eval.variants.registry import VARIANTS
v = VARIANTS['applike']
tools = v.tools_factory()
print(f'Tools: {[t.name for t in tools]}')
print(f'schedule_tool_name: {v.schedule_tool_name}')
print(f'schedule_tool_names: {v.schedule_tool_names}')
print(f'Persona mentions Calendar: {\"Calendar\" in v.persona}')
"
```
Expected: 4 tools [calendar_create_event, reminders_create, calendar_list_events, calendar_update_event]. schedule_tool_names has both scheduling tools. Persona mentions "Calendar".

`uv run ruff check tests/eval/variants/tasks_applike.py` passes.
  </verify>
  <done>App-like variant registered with 4 tools, split one-shot/recurring, app-framed persona. schedule_tool_names field populated for evaluator compatibility.</done>
</task>

<task type="auto">
  <name>Task 2: Create parity matrix and token budget measurement</name>
  <files>
tests/eval/parity_matrix.md
tests/eval/token_budget.py
  </files>
  <action>
**Parity matrix (tests/eval/parity_matrix.md):**
Create a markdown table with one row per capability and one column per variant (baseline, rename, simplify, description_a, description_b, applike). Follow the format from RESEARCH.md.

Capabilities to cover (happy-path only per user decision):
- Create one-shot task (params for each variant)
- Create recurring task (params for each variant)
- List tasks (params for each variant)
- Cancel task (update action)
- Complete task (update action)
- Log progress (update action)
- Retry task (update action + retry_in handling)
- Ask user (update action + question handling)
- Message user (update action + message handling)

For each cell: show the tool name and key params. Mark any param that's "absorbed" differently (e.g., simplify merges delay_seconds into when, applike merges message/question into detail).

Add a "Notes" column explaining any parity difference.

**Token budget script (tests/eval/token_budget.py):**
Implement using `ChatAnthropic.get_num_tokens_from_messages(tools=...)` as specified in RESEARCH.md.

The script should:
1. Import VARIANTS from registry
2. Create a ChatAnthropic instance with EVAL_MODEL
3. For each variant: measure tool definition tokens (total_with_tools - base_no_tools)
4. For each variant: measure system prompt tokens separately
5. Print a comparison table showing: variant name, tool definition tokens, system prompt tokens, total overhead, delta % vs baseline
6. Assert app-like tool definitions are within 10% of baseline (as a design principle check)
7. Use `loguru` for any logging (per CLAUDE.md)

Make it runnable as: `uv run python -m tests.eval.token_budget`

Use the `if __name__ == "__main__":` pattern. Also export a `measure_all_variants() -> dict` function for programmatic use.
  </action>
  <verify>
```bash
uv run python -m tests.eval.token_budget
```
Should print a table showing token counts for all 6 variants with delta percentages. App-like variant should be within 10% of baseline for tool definitions.

`uv run ruff check tests/eval/token_budget.py` passes.

Verify parity_matrix.md exists and has rows for all 9 capabilities x 6 variants.
  </verify>
  <done>Parity matrix documents every parameter mapping across all 6 variants. Token budget script measures and compares token counts. App-like variant confirmed within 10% token budget for tool definitions.</done>
</task>

</tasks>

<verification>
- `uv run python -c "from tests.eval.variants.registry import VARIANTS; print(sorted(VARIANTS.keys()))"` shows `['applike', 'baseline', 'description_a', 'description_b', 'rename', 'simplify']`
- `uv run python -m tests.eval.token_budget` runs and shows all 6 variants with token counts
- `tests/eval/parity_matrix.md` exists with 9 capability rows x 6 variant columns
- `uv run ruff check tests/eval/` passes
- App-like tool definition tokens are within 10% of baseline
</verification>

<success_criteria>
6 variants registered (baseline + 5 experimental). App-like splits one-shot/recurring into separate tools with app-framed persona. Parity matrix confirms no silent capability drops. Token budget measurement confirms the 10% design principle. All files pass linting.
</success_criteria>

<output>
After completion, create `.planning/phases/03-app-like-variant-design/03-02-SUMMARY.md`
</output>
