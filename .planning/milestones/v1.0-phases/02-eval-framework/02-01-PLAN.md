---
phase: 02-eval-framework
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/eval/conftest.py
  - tests/eval/scenarios/tasks_positive.yaml
  - tests/eval/scenarios/tasks_negative.yaml
  - tests/eval/variants/registry.py
  - tests/eval/variants/tasks_baseline.py
autonomous: true
requirements: [EVAL-05]

must_haves:
  truths:
    - "Scenario YAML files load into typed Scenario dataclasses via load_scenarios()"
    - "Variant registry holds named ToolVariant entries with callable tools_factory"
    - "Baseline variant reproduces the current production tool interface"
    - "Adding a new scenario means editing YAML, not Python code"
  artifacts:
    - path: "tests/eval/conftest.py"
      provides: "Scenario loader, Scenario dataclass, ScenarioAssertion dataclass"
      contains: "load_scenarios"
    - path: "tests/eval/scenarios/tasks_positive.yaml"
      provides: "Positive eval scenarios (single, sequence, multi, recurring)"
      contains: "scenarios:"
    - path: "tests/eval/scenarios/tasks_negative.yaml"
      provides: "Negative eval scenarios (should NOT trigger tool calls)"
      contains: "scenarios:"
    - path: "tests/eval/variants/registry.py"
      provides: "ToolVariant dataclass and VARIANTS dict"
      contains: "VARIANTS"
    - path: "tests/eval/variants/tasks_baseline.py"
      provides: "Baseline variant registration"
      contains: "register"
  key_links:
    - from: "tests/eval/conftest.py"
      to: "tests/eval/scenarios/*.yaml"
      via: "yaml.safe_load in load_scenarios()"
      pattern: "yaml\\.safe_load"
    - from: "tests/eval/variants/tasks_baseline.py"
      to: "tests/eval/variants/registry.py"
      via: "register decorator populates VARIANTS dict"
      pattern: "register"
---

<objective>
Build the eval data layer: scenario YAML files, scenario loader, variant registry, and baseline variant.

Purpose: Establish the reusable infrastructure that decouples eval data (scenarios) from eval logic (tests). This is the foundation that makes EVAL-05 (reuse across future experiments) possible -- new experiments add YAML and variant files, not framework code.

Output: Loadable scenarios, typed variant registry, one registered baseline variant matching production tools.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-eval-framework/02-RESEARCH.md

# Reference: existing eval and production tools
@tests/joi_agent_langgraph2/test_task_scheduling_eval.py
@src/joi_agent_langgraph2/tasks/tools.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scenario YAML files and loader</name>
  <files>
    tests/eval/conftest.py
    tests/eval/scenarios/tasks_positive.yaml
    tests/eval/scenarios/tasks_negative.yaml
  </files>
  <action>
Install dependencies: `uv add scipy pyyaml --dev`

Create `tests/eval/scenarios/tasks_positive.yaml` with scenarios derived from the existing eval's ALL_CASES. Include these categories:

- **single**: "remind me to call mom in 5 min" (min_calls=1, assertions: has_timing)
- **sequence**: "count to 3 with 5 sec pauses" (min_calls=3, assertions: staggered_timing, no_run_code), "count to 10 with 5 sec pauses" (min_calls=10), "send me 3 messages, 1 min apart" (min_calls=3)
- **multi**: "remind me at 3pm and 5pm" (min_calls=2, assertions: has_timing)
- **recurring**: "check on me every morning" (min_calls=1, assertions: is_recurring), "review our conversations daily at 11pm" (min_calls=1, assertions: is_recurring)

Create `tests/eval/scenarios/tasks_negative.yaml` with 5 scenarios that should NOT trigger task scheduling:
- greeting: "hey, how are you?"
- question_about_tasks: "what can you do with tasks?"
- past_tense_reminder: "I forgot to call mom yesterday"
- ambiguous_time: "that reminds me of when I used to wake up early"
- task_word_not_scheduling: "this task at work is really boring"

Create `tests/eval/conftest.py` with:
- `Scenario` dataclass: id, prompt, category, expected_tool (str|None), min_calls (int), assertions (list[ScenarioAssertion])
- `ScenarioAssertion` dataclass: type (str), params (dict, default empty)
- `load_scenarios(name: str) -> list[Scenario]`: reads from `scenarios/{name}.yaml`, returns typed list
- `SCENARIOS_DIR` constant pointing to `Path(__file__).parent / "scenarios"`

Register the `eval` pytest marker in pyproject.toml if not already present (it is -- verify only).
  </action>
  <verify>
`uv run python -c "from tests.eval.conftest import load_scenarios; s = load_scenarios('tasks_positive'); print(f'{len(s)} scenarios loaded'); assert len(s) >= 7; print('OK')"` succeeds.

`uv run python -c "from tests.eval.conftest import load_scenarios; s = load_scenarios('tasks_negative'); print(f'{len(s)} scenarios loaded'); assert len(s) >= 5; print('OK')"` succeeds.
  </verify>
  <done>
YAML scenarios load into typed Scenario objects. Positive file has 7+ scenarios across 4 categories. Negative file has 5+ scenarios. Adding a scenario is a YAML edit.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create variant registry and baseline variant</name>
  <files>
    tests/eval/variants/registry.py
    tests/eval/variants/tasks_baseline.py
  </files>
  <action>
Create `tests/eval/variants/registry.py` with:
- `ToolVariant` dataclass: name (str), persona (str), tools_factory (Callable[[], list[BaseTool]]), schedule_tool_name (str, default "schedule_task"), schedule_action (str|None, default None), description (str, default "")
- `VARIANTS: dict[str, ToolVariant]` global dict
- `register(name: str)` decorator that calls the decorated function, stores result in VARIANTS, returns the function unchanged

Create `tests/eval/variants/tasks_baseline.py` with:
- Import `register` from registry, `ToolVariant` from registry
- Import `StructuredTool` from langchain_core.tools, `tool` decorator
- Define the baseline variant matching the current production `schedule_task` tool interface: 5 params (title, description, when, delay_seconds, recurring), same docstring as production `DESC_FIXED` from the existing eval (the "Schedule ONE background task" description). Include `list_tasks` and `update_task` stubs and a `run_code` stub as companion tools.
- Read persona from `settings.persona_path.read_text()` like the existing eval does (import from `joi_agent_langgraph2.config`)
- Register as "baseline" using the decorator

Ensure `tasks_baseline.py` is imported when the variant registry is accessed. Add an `import tests.eval.variants.tasks_baseline` line at the bottom of `registry.py` to auto-register.
  </action>
  <verify>
`uv run python -c "from tests.eval.variants.registry import VARIANTS; v = VARIANTS['baseline']; print(f'baseline: {v.name}, tools: {len(v.tools_factory())}'); assert len(v.tools_factory()) >= 3; print('OK')"` succeeds.
  </verify>
  <done>
VARIANTS["baseline"] exists with a callable tools_factory returning 4 tools (schedule_task, list_tasks, update_task, run_code). The ToolVariant dataclass provides typed access to variant config. New variants register by decorating a function in a new file and importing it.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from tests.eval.conftest import load_scenarios; print(len(load_scenarios('tasks_positive')), len(load_scenarios('tasks_negative')))"` prints "7 5" (or more)
2. `uv run python -c "from tests.eval.variants.registry import VARIANTS; print(list(VARIANTS.keys()))"` prints `['baseline']`
3. `uv run ruff check tests/eval/` passes
</verification>

<success_criteria>
- Scenario loader works and returns typed objects from YAML
- Variant registry holds baseline with working tools_factory
- No Python code changes needed to add a new scenario
- All files pass ruff check
</success_criteria>

<output>
After completion, create `.planning/phases/02-eval-framework/02-01-SUMMARY.md`
</output>
