---
phase: 02-eval-framework
plan: 03
type: execute
wave: 3
depends_on: [02-02]
files_modified:
  - tests/eval/stats.py
  - tests/eval/conftest.py
  - tests/eval/test_tasks.py
autonomous: true
requirements: [EVAL-02, EVAL-04]

must_haves:
  truths:
    - "Bootstrap confidence intervals are computed for success rates and token costs"
    - "Two-variant comparison shows whether difference is statistically significant"
    - "Cost-per-run is computed from token counts using Haiku pricing"
    - "A JSON report file is generated after eval runs with per-variant statistics"
    - "Running `uv run pytest -m eval` produces a report alongside test results"
  artifacts:
    - path: "tests/eval/stats.py"
      provides: "Bootstrap CI computation, variant comparison, report generation"
      contains: "compare_variants"
    - path: "tests/eval/conftest.py"
      provides: "Updated with results collection fixture and report generation hook"
      contains: "generate_report"
  key_links:
    - from: "tests/eval/stats.py"
      to: "scipy.stats"
      via: "bootstrap() for BCa confidence intervals"
      pattern: "scipy\\.stats\\.bootstrap"
    - from: "tests/eval/conftest.py"
      to: "tests/eval/stats.py"
      via: "conftest calls generate_report after test session"
      pattern: "generate_report"
    - from: "tests/eval/test_tasks.py"
      to: "tests/eval/conftest.py"
      via: "eval_results fixture injection and record_eval_result calls"
      pattern: "record_eval_result"
---

<objective>
Build statistical analysis and reporting: bootstrap confidence intervals, variant comparison, cost computation, and auto-generated JSON reports.

Purpose: Without statistical rigor, eval results are anecdotes. This plan delivers EVAL-02 (significance testing) and completes EVAL-04 (cost comparison). The report is the artifact that Phase 4+ experiments will use to make decisions.

Output: `tests/eval/stats.py` with bootstrap CI + comparison functions, and a pytest session-scoped fixture that auto-generates reports.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-eval-framework/02-RESEARCH.md
@.planning/phases/02-eval-framework/02-02-SUMMARY.md

# Reference: research stats examples
@.planning/phases/02-eval-framework/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create statistical analysis module</name>
  <files>tests/eval/stats.py</files>
  <action>
Create `tests/eval/stats.py` with the following functions:

**Cost constants** (Haiku 4.5 pricing):
- `HAIKU_INPUT_COST_PER_TOKEN = 1.0 / 1_000_000` ($1 per 1M input tokens)
- `HAIKU_OUTPUT_COST_PER_TOKEN = 5.0 / 1_000_000` ($5 per 1M output tokens)

**`compute_cost(input_tokens: int, output_tokens: int) -> float`**: Returns USD cost.

**`bootstrap_ci(data: list[float], confidence_level: float = 0.95, n_resamples: int = 9999) -> dict`**:
- Uses `scipy.stats.bootstrap` with `method="BCa"` and `rng=np.random.default_rng(42)` for reproducibility
- Returns dict: mean, ci_low, ci_high, std_error, n_samples
- Handle edge case: if all values identical, return CI = [mean, mean] with std_error=0 (bootstrap fails on zero-variance data)

**`compare_variants(a_scores: list[float], b_scores: list[float], confidence_level: float = 0.95, n_resamples: int = 9999) -> dict`**:
- Compute mean difference (a - b) with bootstrap CI on the difference
- Uses `scipy.stats.bootstrap` with paired data and `mean_difference` statistic
- Returns: mean_a, mean_b, difference, ci_low, ci_high, confidence_level, significant (bool: 0 not in CI), standard_error
- Handle edge case: fewer than 3 samples returns significant=False with a warning field

**`generate_report(results_by_variant: dict[str, list[dict]], output_path: Path) -> dict`**:
- For each variant, compute:
  - success_rate: bootstrap_ci on correct_tool_score values
  - token_usage: bootstrap_ci on total_tokens
  - cost_usd: bootstrap_ci on computed costs
  - n_samples count
- If multiple variants exist, add pairwise comparisons (each pair's success rate difference with significance)
- Write JSON to output_path
- Log summary to loguru
- Return the report dict

Use loguru for all logging. Use numpy arrays for bootstrap input.
  </action>
  <verify>
`uv run ruff check tests/eval/stats.py` passes.

`uv run python -c "
from tests.eval.stats import bootstrap_ci, compare_variants, compute_cost
# Test bootstrap_ci
r = bootstrap_ci([0.8, 0.9, 1.0, 0.7, 0.8])
print(f'CI: [{r[\"ci_low\"]:.2f}, {r[\"ci_high\"]:.2f}], mean={r[\"mean\"]:.2f}')
assert 0.5 < r['ci_low'] < r['mean'] < r['ci_high'] < 1.1

# Test compare
c = compare_variants([1,1,1,1,0], [0,0,1,0,0])
print(f'diff={c[\"difference\"]:.2f}, significant={c[\"significant\"]}')
assert c['significant'] == True

# Test cost
cost = compute_cost(1000, 200)
print(f'cost={cost:.6f}')
assert cost > 0
print('OK')
"` succeeds.
  </verify>
  <done>
bootstrap_ci returns BCa confidence intervals. compare_variants shows significance. compute_cost uses Haiku pricing. Edge cases (identical data, small samples) handled gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add results collection and report generation to conftest</name>
  <files>tests/eval/conftest.py</files>
  <action>
Update `tests/eval/conftest.py` (created in Plan 01) to add results collection and report generation.

Add a **session-scoped** fixture `eval_results` that collects results during the test session:
- Returns a dict[str, list[dict]] keyed by variant name
- Each entry is a list of result dicts with: correct_tool_score, correct_count_score, input_tokens, output_tokens, total_tokens, scenario_id, category

Add a **helper function** `record_eval_result(eval_results: dict, variant_name: str, result: EvalResult, scenario: Scenario)` that appends to the collection.

Add a **session-scoped autouse fixture** (or pytest_sessionfinish hook) that calls `generate_report()` after all tests finish:
- Only generates if eval_results is non-empty
- Writes to `tests/eval/reports/latest.json`
- Creates the reports/ directory if needed
- Logs a summary table to terminal via loguru: variant name, success rate, mean tokens, mean cost

Add `eval_results` fixture to the conftest so test_tasks.py can inject it and call record_eval_result.

Important: The `eval_results` fixture approach requires test_tasks.py to use it. An alternative is a pytest plugin hook pattern. Use whichever is cleaner -- but prefer the fixture approach since it's simpler and more explicit. The test functions in 02-02's test_tasks.py will need to accept and use `eval_results` as a fixture parameter. Document this coupling clearly.
  </action>
  <verify>
`uv run ruff check tests/eval/conftest.py` passes.
`uv run python -c "from tests.eval.conftest import load_scenarios, record_eval_result; print('OK')"` succeeds.
  </verify>
  <done>
Results are collected during pytest session. After all eval tests complete, a JSON report is written to `tests/eval/reports/latest.json` with per-variant bootstrap CIs for success rate, token usage, and cost. Terminal shows a summary table.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire test_tasks.py to feed eval_results fixture</name>
  <files>tests/eval/test_tasks.py</files>
  <action>
Update `tests/eval/test_tasks.py` (created in Plan 02) to integrate with the results collection from this plan:

1. Add `eval_results` as a fixture parameter to both `test_positive` and `test_negative` functions.
2. After each successful evaluation in `test_positive`, call `record_eval_result(eval_results, variant_name, result, scenario)` to feed the collection.
3. After each assertion in `test_negative`, call `record_eval_result(eval_results, variant_name, ...)` with a manually constructed result dict containing: correct_tool_score=1.0 (if no false trigger), correct_count_score=1.0, and the token counts from the response.

This wiring ensures `generate_report()` in the session-finish hook has data to produce the JSON report.
  </action>
  <verify>
`uv run ruff check tests/eval/test_tasks.py` passes.

`uv run python -c "import ast; tree = ast.parse(open('tests/eval/test_tasks.py').read()); funcs = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef) and n.name in ('test_positive', 'test_negative')]; assert all('eval_results' in [a.arg for a in f.args.args] for f in funcs); print('Both test functions accept eval_results fixture')"` succeeds.
  </verify>
  <done>
Both `test_positive` and `test_negative` accept the `eval_results` fixture and call `record_eval_result` after each evaluation. Running `uv run pytest -m eval` populates the results collection and triggers report generation.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check tests/eval/` passes
2. `uv run python -c "from tests.eval.stats import bootstrap_ci, compare_variants, generate_report; print('OK')"` succeeds
3. (If API key available) `uv run pytest -m eval -k "baseline" --count=3 -x` generates `tests/eval/reports/latest.json`
4. The generated report contains success_rate, token_usage, and cost_usd sections per variant
</verification>

<success_criteria>
- Bootstrap CIs computed with scipy BCa method
- Variant comparison correctly identifies significance (0 not in CI)
- Cost computed from Haiku pricing constants
- Report auto-generated as JSON after eval session
- Summary logged to terminal
</success_criteria>

<output>
After completion, create `.planning/phases/02-eval-framework/02-03-SUMMARY.md`
</output>
