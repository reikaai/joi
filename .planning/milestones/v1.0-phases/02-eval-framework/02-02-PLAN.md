---
phase: 02-eval-framework
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - tests/eval/evaluators.py
  - tests/eval/test_tasks.py
  - tests/eval/cache/.gitkeep
  - .gitignore
autonomous: true
requirements: [EVAL-01, EVAL-03, EVAL-04]

must_haves:
  truths:
    - "Positive scenarios assert correct tool selection, call count, and timing behavior"
    - "Negative scenarios assert zero scheduling tool calls"
    - "Token usage (input, output, total) is captured for every eval run"
    - "Results are tracked in LangSmith as named experiments via @pytest.mark.langsmith"
    - "Running `uv run pytest -m eval` executes the full eval suite"
  artifacts:
    - path: "tests/eval/evaluators.py"
      provides: "Reusable evaluation functions: evaluate_tool_calls, assertion helpers"
      contains: "evaluate_tool_calls"
    - path: "tests/eval/test_tasks.py"
      provides: "Parametrized eval tests for positive and negative scenarios across variants"
      contains: "test_positive"
  key_links:
    - from: "tests/eval/test_tasks.py"
      to: "tests/eval/conftest.py"
      via: "load_scenarios() for parametrization"
      pattern: "load_scenarios"
    - from: "tests/eval/test_tasks.py"
      to: "tests/eval/variants/registry.py"
      via: "VARIANTS dict for variant iteration"
      pattern: "VARIANTS"
    - from: "tests/eval/test_tasks.py"
      to: "tests/eval/evaluators.py"
      via: "evaluate_tool_calls() for structured evaluation"
      pattern: "evaluate_tool_calls"
    - from: "tests/eval/test_tasks.py"
      to: "langsmith"
      via: "@pytest.mark.langsmith + t.log_feedback for experiment tracking"
      pattern: "pytest\\.mark\\.langsmith"
---

<objective>
Build the eval test file with LangSmith tracking, evaluator logic, and positive/negative test functions.

Purpose: This is the core eval engine. It wires scenarios to variants to LLM calls to assertions. LangSmith integration (EVAL-01) provides experiment tracking. Negative cases (EVAL-03) test for false triggers. Token cost capture (EVAL-04) is logged per-run via LangSmith feedback.

Output: A runnable `uv run pytest -m eval` that executes all scenario/variant combinations, tracks results in LangSmith, captures token costs, and asserts correctness.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-eval-framework/02-RESEARCH.md
@.planning/phases/02-eval-framework/02-01-SUMMARY.md

# Reference: existing eval patterns and production tools
@tests/joi_agent_langgraph2/test_task_scheduling_eval.py
@src/joi_agent_langgraph2/tasks/tools.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create evaluators module with reusable assertion logic</name>
  <files>tests/eval/evaluators.py</files>
  <action>
Create `tests/eval/evaluators.py` that extracts and generalizes the assertion logic from the existing `test_task_scheduling_eval.py`. This module provides reusable evaluation functions that work with any variant.

Define an `EvalResult` dataclass:
- tool_call_names: list[str]
- call_count: int
- correct_tool_score: float (1.0 or 0.0)
- correct_count_score: float (1.0 or 0.0)
- input_tokens: int
- output_tokens: int
- total_tokens: int
- passed: bool
- failure_message: str

Implement `evaluate_tool_calls(response, scenario: Scenario, variant: ToolVariant) -> EvalResult`:
1. Extract tool calls from response matching variant.schedule_tool_name (filter by schedule_action if set)
2. Extract token usage from response.usage_metadata (assert not None)
3. Check correct tool: did the LLM call the right tool?
4. Check correct count: >= scenario.min_calls?
5. Check no run_code fallback (if scenario has no_run_code assertion)
6. Check staggered timing (if scenario has staggered_timing assertion) -- port the `_assert_staggered` logic from existing eval
7. Check has_timing (if scenario has has_timing assertion)
8. Check is_recurring (if scenario has is_recurring assertion)
9. Build passed = all checks pass, failure_message = first failure

Port the helper functions from the existing eval:
- `_looks_like_cron(value: str) -> bool`
- Staggered timing validation (delay_seconds strictly increasing, or distinct when values)
- Has-timing validation (each call has delay_seconds or when)
- Recurring validation (recurring=True or cron-like when)

Use loguru for any diagnostic logging (never print).
  </action>
  <verify>
`uv run ruff check tests/eval/evaluators.py` passes.
`uv run python -c "from tests.eval.evaluators import evaluate_tool_calls, EvalResult; print('OK')"` succeeds.
  </verify>
  <done>
evaluate_tool_calls() returns structured EvalResult with scores, token counts, and pass/fail verdict. All assertion types from the existing eval are covered: staggered_timing, has_timing, is_recurring, no_run_code.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create eval test file with LangSmith tracking</name>
  <files>tests/eval/test_tasks.py</files>
  <action>
Create `tests/eval/test_tasks.py` -- the main eval test file.

Define module-level constants:
- `EVAL_MODEL = "claude-haiku-4-5-20241022"` -- hardcoded for eval consistency (per research recommendation)

Define `invoke_variant(variant: ToolVariant, prompt: str)` async helper:
- Creates `ChatAnthropic(model=EVAL_MODEL, api_key=settings.anthropic_api_key)`
- Binds variant's tools via `bind_tools(variant.tools_factory())`
- Invokes with `[SystemMessage(content=variant.persona), HumanMessage(content=f"[{timestamp}]\n{prompt}")]`
- Uses current UTC timestamp in the message (like existing eval does)
- Returns the AIMessage response

Implement `test_positive`:
- Markers: `@pytest.mark.langsmith`, `@pytest.mark.eval`, `@pytest.mark.asyncio`
- Parametrize over variant names (from VARIANTS.keys()) and scenarios (from load_scenarios("tasks_positive"))
- Log inputs via `t.log_inputs({"prompt": ..., "variant": ..., "category": ...})`
- Log reference outputs via `t.log_reference_outputs({"expected_tool": ..., "min_calls": ...})`
- Call invoke_variant, then evaluate_tool_calls
- Log outputs: tool_call_names, call_count
- Log feedback: correct_tool (score), correct_count (score), input_tokens (value), output_tokens (value), total_tokens (value)
- Assert result.passed with result.failure_message

Implement `test_negative`:
- Same markers as positive
- Parametrize over variant names and scenarios (from load_scenarios("tasks_negative"))
- Log inputs with category="negative"
- Log reference outputs with expected_tool_calls=0
- Call invoke_variant
- Count scheduling tool calls (variant.schedule_tool_name, filtered by schedule_action if set)
- Log feedback: no_false_trigger (score 1.0 if zero calls, else 0.0), input_tokens, output_tokens
- Assert zero scheduling calls

Add the `langsmith` marker to `pyproject.toml` markers list if not present.

Import variant modules to ensure auto-registration: add `import tests.eval.variants.tasks_baseline` at top of test file (or rely on registry.py's bottom import).

**Dual-mode: real vs cached (EVAL-05 locked decision):**

Implement a `LANGSMITH_TEST_CACHE` env-var-gated path in `invoke_variant`:
1. Define a cache directory: `tests/eval/cache/` (gitignored except .gitkeep).
2. When `LANGSMITH_TEST_CACHE` is unset or empty: make real LLM calls (the default path above).
3. When `LANGSMITH_TEST_CACHE=read`: before calling the LLM, check for a cached response file at `tests/eval/cache/{variant_name}/{scenario_id}.json`. If found, deserialize it (reconstruct AIMessage with tool_calls and usage_metadata) and return without making an LLM call. If not found, make the real call and save the response to that path.
4. When `LANGSMITH_TEST_CACHE=write`: always make the real call and write the response to the cache file (overwriting if exists). This is used to refresh baselines.

The cache file format: JSON with keys `tool_calls` (list of dicts), `content` (str), `usage_metadata` (dict with input_tokens, output_tokens, total_tokens). Use `json.dumps`/`json.loads` with a simple serialization of AIMessage fields.

Add `tests/eval/cache/` to `.gitignore` with a `tests/eval/cache/.gitkeep` to preserve the directory.

This satisfies the locked decision: "real calls for active experiments, cached for regression/established baselines."
  </action>
  <verify>
`uv run ruff check tests/eval/test_tasks.py` passes.

Dry-run collection (no actual LLM calls): `uv run pytest tests/eval/test_tasks.py --collect-only -q` shows test cases parametrized across variants and scenarios. Should show at least 12 items (7 positive + 5 negative for baseline variant).

If `ANTHROPIC_API_KEY` is set, a single targeted run: `uv run pytest tests/eval/test_tasks.py -k "test_positive[baseline-single_reminder]" --count=1 -x` should pass.

Verify cache write: `LANGSMITH_TEST_CACHE=write uv run pytest tests/eval/test_tasks.py -k "test_positive[baseline-single_reminder]" --count=1 -x` creates a JSON file under `tests/eval/cache/baseline/`.

Verify cache read: `LANGSMITH_TEST_CACHE=read uv run pytest tests/eval/test_tasks.py -k "test_positive[baseline-single_reminder]" --count=1 -x` passes without making an LLM call (fast execution).
  </verify>
  <done>
`uv run pytest -m eval --collect-only` lists all scenario/variant combinations. Test functions log inputs, outputs, and feedback to LangSmith. Positive tests assert tool correctness. Negative tests assert no false triggers. Token costs are captured in every run.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/eval/test_tasks.py --collect-only -q` shows parametrized test IDs
2. `uv run ruff check tests/eval/` passes
3. (If API key available) `uv run pytest tests/eval/test_tasks.py -k "baseline" -x --count=1` runs and passes
</verification>

<success_criteria>
- `uv run pytest -m eval` is the entry point for running evals
- LangSmith feedback includes correct_tool, correct_count, input_tokens, output_tokens, total_tokens
- Negative tests assert zero scheduling calls
- Dual-mode implemented: `LANGSMITH_TEST_CACHE=read` uses cached responses, `LANGSMITH_TEST_CACHE=write` refreshes cache, unset = real calls
</success_criteria>

<output>
After completion, create `.planning/phases/02-eval-framework/02-02-SUMMARY.md`
</output>
