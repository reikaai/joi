---
phase: 06-adr-and-decision
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/adr-tool-interface-experiment.md
autonomous: true
requirements: [DOCS-01]

must_haves:
  truths:
    - "ADR documents the hypothesis, methodology, results, and decision in a self-contained way"
    - "Statistical results include actual numbers: confidence intervals, effect sizes, p-values, token costs"
    - "A reader who was not involved in the experiment can understand the full story"
    - "The decision section clearly states REJECT with specific conditions under which to revisit"
    - "'Why It Didn't Work' section explains root causes for the null/negative result"
    - "'What Would Need To Be True' section outlines conditions where tool redesign would matter"
  artifacts:
    - path: "docs/adr-tool-interface-experiment.md"
      provides: "Architecture Decision Record for tool interface experiment"
      min_lines: 150
  key_links:
    - from: "docs/adr-tool-interface-experiment.md"
      to: ".planning/phases/05-full-comparison/EXPLORATION.md"
      via: "references statistical data from the exploration notebook"
      pattern: "p=0\\.006|36\\.7%|660 LLM"
---

<objective>
Write the Architecture Decision Record documenting the tool interface experiment: hypothesis, methodology, results, decision, and consequences.

Purpose: Create a permanent record that informs future Joi development and serves as a portfolio artifact. Goes beyond standard ADR format to include root cause analysis ("Why It Didn't Work") and generalization conditions ("What Would Need To Be True").

Output: `docs/adr-tool-interface-experiment.md`
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Primary data sources -- read these thoroughly before writing
@.planning/phases/05-full-comparison/EXPLORATION.md
@.planning/phases/04-isolated-variable-experiments/04-02-SUMMARY.md

# Existing ADR for format reference
@docs/adr-dev-workflow-tooling.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write ADR for tool interface experiment</name>
  <files>docs/adr-tool-interface-experiment.md</files>
  <action>
Create `docs/adr-tool-interface-experiment.md` with the following structure. Write as a researcher presenting to peers -- clear, honest about limitations, specific about evidence. Portfolio-quality writing.

**Header:**
- Title: "ADR: Tool Interface Design for LLM Task Scheduling"
- Status: DECIDED -- Retain programmatic interface
- Date: 2026-02-19
- Context: Joi agent, Haiku 4.5

**Sections (in order):**

1. **Problem Statement** (2-3 paragraphs)
   - The "apps vs tools" hypothesis: should LLM tool interfaces mirror human app paradigms (Calendar, Reminders) or programmatic APIs (schedule_task with flags)?
   - Why this matters for Joi: personal assistant agent, tool count will grow, interface patterns set precedent.
   - Scope: task scheduling domain only, Haiku 4.5 model.

2. **Hypothesis**
   - Decomposing `schedule_task` into semantically distinct tools (Calendar for one-time, Reminders for recurring) would improve routing accuracy by matching user mental models.
   - Controlled for: naming (rename-only), parameter simplification (simplify-only), descriptions (description-only), combined (full app-like).

3. **Methodology** (structured, reproducible)
   - Eval framework: YAML scenarios, typed variants, bootstrap CI + Fisher exact tests.
   - Phase 4: Isolated variable experiments -- 300 LLM calls, 5 variants, 12 scenarios, 5 reps.
   - Phase 5: Full comparison -- 660 LLM calls, 3 exploration pivots, 26 scenarios (17 positive + 9 negative).
   - Iterative exploration: easy scenarios first (ceiling calibration), then hard scenarios across 4 difficulty dimensions (ambiguous, multi, distractor, implicit).
   - Include the table from EXPLORATION.md "Statistical Summary" section.

4. **Results** (data-heavy, tables)
   - Phase 4 isolated variables: all NS. Table with variant/rate/CI/delta.
   - Phase 5 Pivot 0 (easy): 98.3% vs 93.3%, NS. Fisher p=0.364.
   - Phase 5 Pivot 2 (hard): 69.0% vs 53.0% on hard positive, p=0.029. hard_ambiguous: 53.3% vs 16.7%, p=0.006.
   - Cost comparison: negligible (<1%).
   - Include the cumulative data table from EXPLORATION.md.
   - Additive null model analysis: predicted 88.4%, actual 93.3% on easy (synergy), but negative compounding on hard.

5. **Decision**
   - REJECT the app-like variant for task scheduling with Haiku 4.5.
   - Retain `schedule_task` with `recurring` flag as the production interface.
   - State clearly: this applies to current model + current domain. Not a universal claim.

6. **Why It Didn't Work** (root cause analysis -- this is the unique section)
   Four root causes, each with evidence:
   a) **Haiku 4.5 is near-optimal for structured tool use (~95% ceiling)**: At the baseline success rate, there is no headroom for improvement. Isolated changes can only show noise or degradation.
   b) **Tool decomposition adds a routing failure mode**: Splitting one tool into two forces the LLM to make a routing decision. Under ambiguity, Haiku 4.5 freezes -- it asks for clarification, picks the wrong tool, or skips scheduling entirely. The 36.7% gap on hard_ambiguous (p=0.006) is direct evidence.
   c) **The hypothesis targeted the wrong layer**: The experiment tested tool discovery/routing, but the actual failure modes are about intent inference (does the user want scheduling?) and parameter extraction (when exactly?). Renaming tools doesn't help when the LLM can't parse "in a bit" into a concrete time.
   d) **Model-specific ceiling**: Haiku 4.5 is specifically optimized for tool use. A weaker or less fine-tuned model might benefit more from clearer tool semantics.

7. **What Would Need To Be True** (generalization conditions)
   Under what conditions WOULD tool interface redesign matter?
   a) **More tools (20+)**: With 3-4 scheduling tools, routing is trivial. At 20+ tools, discovery becomes a bottleneck and semantic naming could help.
   b) **Weaker model**: A model with lower baseline tool accuracy (70-80%) would have headroom for naming improvements to close.
   c) **Different task domain**: Scheduling maps to one semantic dimension (time). Domains with natural multi-category structure (e.g., media: movies/shows/music) might benefit from decomposition.
   d) **Multi-turn with error recovery**: Single-turn eval misses whether app-like framing helps the LLM self-correct after a wrong tool choice.
   e) **User-facing tool selection**: If tools are exposed to users (not just LLMs), human-friendly naming has independent value.

8. **Consequences**
   - Future tool design defaults to consolidated interfaces (fewer tools with flags) over decomposed interfaces.
   - This is a model-specific, domain-specific finding -- revisit with Sonnet/Opus or when tool count exceeds 15.
   - Eval framework and methodology are reusable for future experiments.
   - Total experiment cost: $2.25 across 660 LLM calls.

9. **Limitations**
   - Single model (Haiku 4.5), single domain (task scheduling), single-turn only.
   - Hard scenarios were designed to stress-test specific dimensions -- real user behavior may differ.
   - Sample sizes adequate for large effects but may miss subtle improvements (< 5%).

10. **Open Questions**
    - Does the finding hold for Sonnet/Opus?
    - Would a hybrid (app naming + single tool) perform differently?
    - Does the routing tax scale linearly with tool count?
    - What is the crossover point where tool count makes decomposition beneficial?

**Writing guidelines:**
- Use markdown tables for all statistical data.
- Every claim must cite specific numbers (not "it was better" but "53.3% vs 16.7%, Fisher p=0.006").
- Tone: measured, honest, researcher-to-researcher. Acknowledge limitations upfront.
- No emojis. No marketing language. Let the data speak.
- Target 200-300 lines.
  </action>
  <verify>
Verify the ADR:
1. `wc -l docs/adr-tool-interface-experiment.md` shows >= 150 lines
2. `grep -c "p=" docs/adr-tool-interface-experiment.md` shows >= 5 (statistical references)
3. `grep "Why It Didn" docs/adr-tool-interface-experiment.md` finds the root cause section
4. `grep "What Would Need" docs/adr-tool-interface-experiment.md` finds the generalization section
5. `grep "REJECT\|DECIDED" docs/adr-tool-interface-experiment.md` finds the decision
6. `grep "660\|36\.7%\|0\.006" docs/adr-tool-interface-experiment.md` finds key data points
  </verify>
  <done>
ADR exists at docs/adr-tool-interface-experiment.md with all 10 sections. Contains actual statistical data (CIs, p-values, effect sizes). Decision clearly states REJECT with model/domain scope. "Why It Didn't Work" explains 4 root causes with evidence. "What Would Need To Be True" lists 5 generalization conditions. Readable by someone unfamiliar with the experiment.
  </done>
</task>

</tasks>

<verification>
1. ADR file exists at `docs/adr-tool-interface-experiment.md`
2. All 10 sections present: Problem Statement, Hypothesis, Methodology, Results, Decision, Why It Didn't Work, What Would Need To Be True, Consequences, Limitations, Open Questions
3. Statistical tables include actual numbers (not placeholders)
4. The document is self-contained -- a reader needs no other files to understand the full story
5. Portfolio quality -- clear writing, honest limitations, specific evidence
</verification>

<success_criteria>
- ADR is >= 150 lines with all 10 sections
- Every statistical claim cites specific numbers with p-values where applicable
- Decision section clearly states REJECT with scope boundaries
- Root cause analysis provides 4 distinct explanations with evidence
- Generalization section provides 5 conditions for revisiting the decision
- Document reads coherently from top to bottom without needing to reference other files
</success_criteria>

<output>
After completion, create `.planning/phases/06-adr-and-decision/06-01-SUMMARY.md`
</output>
