---
phase: 04-isolated-variable-experiments
plan: 02
type: execute
wave: 2
depends_on:
  - "04-01"
files_modified:
  - tests/eval/reports/latest.json
  - tests/eval/reports/phase4_summary.md
autonomous: true
requirements:
  - EXPR-02
must_haves:
  truths:
    - "Baseline variant has been run with 5 repetitions and success rate CI is reported"
    - "Rename-only variant has been compared to baseline with bootstrap CI"
    - "Simplify-only variant has been compared to baseline with bootstrap CI"
    - "Description_a variant has been compared to baseline with bootstrap CI"
    - "Description_b variant has been compared to baseline with bootstrap CI"
    - "Results clearly indicate which variable(s) produce significant improvement or show no difference with CI interpretation"
  artifacts:
    - path: "tests/eval/reports/latest.json"
      provides: "Raw JSON report with per-variant CIs and pairwise comparisons"
      contains: "comparisons"
    - path: "tests/eval/reports/phase4_summary.md"
      provides: "Human-readable markdown summary table for Phase 5 consumption"
      contains: "significant"
  key_links:
    - from: "tests/eval/reports/phase4_summary.md"
      to: "tests/eval/reports/latest.json"
      via: "Derived from JSON report data"
      pattern: "ci_low.*ci_high"
---

<objective>
Run the isolated variable experiments and generate an interpretable comparison report.

Purpose: This is the core deliverable of Phase 4 — actual experimental data showing whether renaming, simplifying, or rewriting descriptions improves LLM tool-use accuracy. The results feed directly into Phase 5 (combined comparison) and Phase 6 (ADR).

Output: JSON report with statistical comparisons + markdown summary table.
</objective>

<execution_context>
@/Users/iorlas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/iorlas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-isolated-variable-experiments/04-RESEARCH.md
@.planning/phases/04-isolated-variable-experiments/04-01-SUMMARY.md
@tests/eval/conftest.py
@tests/eval/stats.py
@tests/eval/test_tasks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run 5-variant experiment with statistical repetitions</name>
  <files>tests/eval/reports/latest.json</files>
  <action>
Run the full isolated-variable experiment. CRITICAL: Do NOT use cache mode — every run must make real LLM calls for statistical validity.

```bash
# Ensure no cache interference
unset LANGSMITH_TEST_CACHE

# Run: 5 variants x 12 scenarios x 5 reps = 300 LLM calls (~$0.90)
uv run pytest -m eval --count=5 \
  -k "baseline or rename or simplify or description_a or description_b" \
  -v --tb=short 2>&1 | tee eval_results_phase4.txt
```

The autouse session fixture in conftest.py will automatically generate `tests/eval/reports/latest.json` with:
- Per-variant success rates with bootstrap CIs
- Per-variant token usage and cost CIs
- All pairwise comparisons with significance flags

If any variant shows 0% across all scenarios or produces evaluator errors, STOP and check if Plan 01 fixes were applied correctly.

If CIs are very wide (>15% spread) on key comparisons, consider re-running with `--count=10` for tighter bounds. Cost: ~$1.80 total for 10 reps.

Note on pytest-repeat behavior: verify that results collection works correctly with --count by checking that eval_results has 60 entries per variant (12 scenarios * 5 reps), not 12.
  </action>
  <verify>
```bash
# Verify report was generated
test -f tests/eval/reports/latest.json && echo "Report exists" || echo "MISSING"

# Verify all 5 variants present in report
python3 -c "import json; r=json.load(open('tests/eval/reports/latest.json')); print('Variants:', list(r['variants'].keys())); print('Comparisons:', len(r['comparisons']))"

# Verify sample counts (should be 60 per variant = 12 scenarios * 5 reps)
python3 -c "import json; r=json.load(open('tests/eval/reports/latest.json')); [print(f'{k}: n={v[\"n_samples\"]}') for k,v in r['variants'].items()]"
```
  </verify>
  <done>
- latest.json contains results for all 5 variants (baseline, rename, simplify, description_a, description_b)
- Each variant has >= 60 samples (12 scenarios * 5 reps)
- All 10 pairwise comparisons computed (C(5,2) = 10)
- No variant shows 0% success rate (would indicate evaluator bug)
  </done>
</task>

<task type="auto">
  <name>Task 2: Generate human-readable Phase 4 summary report</name>
  <files>tests/eval/reports/phase4_summary.md</files>
  <action>
Read the `tests/eval/reports/latest.json` and create a markdown summary at `tests/eval/reports/phase4_summary.md`.

The summary must contain:

1. **Experiment metadata**: date, model, repetitions, total LLM calls, total cost
2. **Per-variant results table**:
   ```
   | Variant | N | Success Rate | CI (95%) | Avg Tokens | Avg Cost |
   |---------|---|--------------|----------|------------|----------|
   | baseline | 60 | 92.0% | [85.0%, 97.0%] | 1234 | $0.003 |
   ```
3. **Baseline comparisons table** (the 4 key comparisons):
   ```
   | Variant vs Baseline | Difference | CI (95%) | Significant? | Interpretation |
   |---------------------|------------|----------|--------------|----------------|
   | rename | +3.0% | [-2.0%, +8.0%] | No | No detectable effect (narrow CI) |
   ```
4. **Interpretation section** for each comparison:
   - If significant + positive: "X improves accuracy by Y%"
   - If significant + negative: "X hurts accuracy by Y%"
   - If not significant + narrow CI: "No meaningful effect detected"
   - If not significant + wide CI: "Inconclusive — insufficient power"
5. **Description A vs B comparison** (head-to-head, not just vs baseline)
6. **Token cost analysis**: which variants are cheaper/more expensive than baseline
7. **Key findings summary**: 2-3 bullet points for Phase 5 consumption

Use the exact numbers from latest.json. Do NOT fabricate or round aggressively — show 1 decimal place for percentages, 3 decimal places for differences.

Run `ruff check` on any Python scripts if created.
  </action>
  <verify>
```bash
# Verify summary was created
test -f tests/eval/reports/phase4_summary.md && echo "Summary exists" || echo "MISSING"

# Verify it contains key sections
grep -c "Significant" tests/eval/reports/phase4_summary.md
grep -c "Interpretation" tests/eval/reports/phase4_summary.md
```
  </verify>
  <done>
- phase4_summary.md exists with all 7 sections populated
- All 4 baseline comparisons have interpretation (significant/not significant + CI width assessment)
- Description A vs B head-to-head comparison included
- Token cost analysis included
- Key findings section has 2-3 actionable bullets for Phase 5
  </done>
</task>

</tasks>

<verification>
1. `tests/eval/reports/latest.json` exists with 5 variants and 10 comparisons
2. `tests/eval/reports/phase4_summary.md` exists with interpretable results
3. Each variant has >= 60 samples (5 reps per scenario)
4. At least one comparison pair has a clear interpretation (significant or narrow-CI non-significant)
5. Token costs reported alongside accuracy
</verification>

<success_criteria>
- All 5 isolated variants have been compared to baseline with bootstrap CIs
- Results clearly show which variable(s) produce statistically significant improvement
- Human-readable summary ready for Phase 5 consumption
- No evaluator artifacts — all results reflect real LLM behavior
</success_criteria>

<output>
After completion, create `.planning/phases/04-isolated-variable-experiments/04-02-SUMMARY.md`
</output>
